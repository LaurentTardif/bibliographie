= HBase  
:toc:

== What's HBASE

A non-relational (NoSQL) database that runs on top of HDFS

Apache HBase is an open source NoSQL database that provides real-time read/write access to those large datasets.

HBase scales linearly to handle huge data sets with billions of rows and millions of columns, and it easily combines data sources that use a wide variety of different structures and schemas. HBase is natively integrated with Hadoop and works seamlessly alongside other data access engines through YARN.
What HBase Does

Apache HBase provides random, real time access to your data in Hadoop. It was created for hosting very large tables, making it a great choice to store multi-structured or sparse data. Users can query HBase for a particular point in time, making “flashback” queries possible. These following characterisitcs make HBase a great choice for storing semi-structured data like log data and then providing that data very quickly to users or applications integrated with HBase.

 * Fault tolerant 	

	Replication across the data center
    Atomic and strongly consistent row-level operations
    High availability through automatic failover
    Automatic sharding and load balancings of tables

 * Fast 	

    Near real time lookups
    In-memory caching via block cache and bloom filters
    Server side processing via filters and co-processors
	
 * Usable 	
 
    Data model accommodates wide range of use cases
    Metrics exports via File and Ganglia plugins
    Easy Java API as well as Thrift and REST gateway APIs


== Versions 

 * 2.0.0 (soon)
 * 1.3.0 (latest)
 * 1.2.4 (stable)

== Introduction 

HBase quick start, good practices 

From official references 

 * http://hbase.apache.org/book.html
 * Download link : http://www.apache.org/dyn/closer.cgi/hbase/

From tutorials

 *  http://www.guru99.com/hbase-tutorials.html
 * http://hortonworks.com/apache/hbase/
 * https://hbase.apache.org/book.html
 
From Pro and cons
 
 * https://www.packtpub.com/mapt/book/big-data-and-business-intelligence/9781783985944/1/ch01lvl1sec16/hbase-pros-and-cons
 * http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part-3-pros-cons/
 * http://stackoverflow.com/questions/22542307/hbase-what-are-the-pros-and-cons-of-using-one-column-with-a-list-of-values-vs
 * http://www.slideshare.net/EdurekaIN/no-sql-databases-35591065

Data Model 

 * http://jimbojw.com/#understanding%20hbase
 * http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/9353-login1210_khurana.pdf
 
Performance

 * https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines
 
Tips 

 * https://www.dynamicyield.com/2015/05/apache-hbase-for-the-win-2/
 * http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/
 * http://www.techsquids.com/bd/hbase-scan-filters-tips-tricks/
 * http://lecluster.delaurent.com/hbase-tips-tricks/
 * https://intellipaat.com/interview-question/hbase-interview-questions/
 * https://dzone.com/articles/handling-big-data-hbase-part-5
 * http://www.slideshare.net/lhofhansl/h-base-tuninghbasecon2015ok

== Key concepts

=== Master, zookeeper, region servers


=== Database and datamodel 

==== namespace
 
A namespace is a logical grouping of tables analogous to a database in relation database systems. This abstraction lays the groundwork for upcoming multi-tenancy related features:

 * Quota Management (HBASE-8410) - Restrict the amount of resources (i.e. regions, tables) a namespace can consume.
 * Namespace Security Administration (HBASE-9206) - Provide another level of security administration for tenants.
 * Region server groups (HBASE-6721) - A namespace/table can be pinned onto a subset of RegionServers thus guaranteeing a course level of isolation.
 
==== Column family

Columns in Apache HBase are grouped into column families. All column members of a column family have the same prefix. For example, the columns courses:history and courses:math are both members of the courses column family. The colon character (:) delimits the column family from the column family qualifier. The column family prefix must be composed of printable characters. The qualifying tail, the column family qualifier, can be made of any arbitrary bytes. Column families must be declared up front at schema definition time whereas columns do not need to be defined at schema time but can be conjured on the fly while the table is up and running.
Physically, all column family members are stored together on the filesystem. Because tunings and storage specifications are done at the column family level, it is advised that all column family members have the same general access pattern and size characteristics.
 
==== Table, Row, Cell
 
 * Tables are declared up front at schema definition time
 * Row keys are uninterpreted bytes. Rows are lexicographically sorted with the lowest order appearing first in a table. The empty byte array is used to denote both the start and end of a tables' namespace.
 * A {row, column, version} tuple exactly specifies a cell in HBase. Cell content is uninterpreted bytes

A {row, column, version} tuple exactly specifies a cell in HBase. It’s possible to have an unbounded number of cells where the row and column are the same but the cell address differs only in its version dimension.

While rows and column keys are expressed as bytes, the version is specified using a long integer. Typically this long contains time instances such as those returned by java.util.Date.getTime() or System.currentTimeMillis(), that is: the difference, measured in milliseconds, between the current time and midnight, January 1, 1970 UTC.

The HBase version dimension is stored in decreasing order, so that when reading from a store file, the most recent values are found first.

There is a lot of confusion over the semantics of cell versions, in HBase. In particular:

 * If multiple writes to a cell have the same version, only the last written is fetchable.
 * It is OK to write cells in a non-increasing version order.
 
[TIP]
====
The maximum number of versions to store for a given column is part of the column schema and is specified at table creation, or via an alter command, via HColumnDescriptor.DEFAULT_VERSIONS. 
==== 

[TIP]
.Modify the Maximum Number of Versions for a Column Family
====
alter ‘t1′, NAME => ‘f1′, VERSIONS => 5
====
 
[TIP]
.Modify the Maximum Number of Versions for a Column Family
====
alter ‘t1′, NAME => ‘f1′, MIN_VERSIONS => 2
====
 
=== Understanding HBASE and BigTable (from Jim R. Wilson)

May 2008

The hardest part about learning HBase (the open source implementation of Google's BigTable), is just wrapping your mind around the concept of what it actually is.

I find it rather unfortunate that these two great systems contain the words table and base in their names, which tend to cause confusion among RDBMS indoctrinated individuals (like myself).

This article aims to describe these distributed data storage systems from a conceptual standpoint. After reading it, you should be better able to make an educated decision regarding when you might want to use HBase vs when you'd be better off with a "traditional" database.
it's all in the terminology

Fortunately, Google's BigTable Paper clearly explains what BigTable actually is. Here is the first sentence of the "Data Model" section:

    A Bigtable is a sparse, distributed, persistent multidimensional sorted map.

Note: At this juncture I like to give readers the opportunity to collect any brain matter which may have left their skulls upon reading that last line.

The BigTable paper continues, explaining that:

    The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes.

Along those lines, the HBaseArchitecture page of the Hadoop wiki posits that:

    HBase uses a data model very similar to that of Bigtable. Users store data rows in labelled tables. A data row has a sortable key and an arbitrary number of columns. The table is stored sparsely, so that rows in the same table can have crazily-varying columns, if the user likes.

Although all of that may seem rather cryptic, it makes sense once you break it down a word at a time. I like to discuss them in this sequence: map, persistent, distributed, sorted, multidimensional, and sparse.

Rather than trying to picture a complete system all at once, I find it easier to build up a mental framework piecemeal, to ease into it...

==== map

At its core, HBase/BigTable is a map. Depending on your programming language background, you may be more familiar with the terms associative array (PHP), dictionary (Python), Hash (Ruby), or Object (JavaScript).

From the wikipedia article, a map is "an abstract data type composed of a collection of keys and a collection of values, where each key is associated with one value."

Using JavaScript Object Notation, here's an example of a simple map where all the values are just strings:

[source,json,subs="verbatim,attributes"]
----
{
  "zzzzz" : "woot",
  "xyz" : "hello",
  "aaaab" : "world",
  "1" : "x",
  "aaaaa" : "y"
}
----

==== persistent

Persistence merely means that the data you put in this special map "persists" after the program that created or accessed it is finished. This is no different in concept than any other kind of persistent storage such as a file on a filesystem. Moving along...

==== distributed

HBase and BigTable are built upon distributed filesystems so that the underlying file storage can be spread out among an array of independent machines.

HBase sits atop either Hadoop's Distributed File System (HDFS) or Amazon's Simple Storage Service (S3), while a BigTable makes use of the Google File System (GFS).

Data is replicated across a number of participating nodes in an analogous manner to how data is striped across discs in a RAID system.

For the purpose of this article, we don't really care which distributed filesystem implementation is being used. The important thing to understand is that it is distributed, which provides a layer of protection against, say, a node within the cluster failing.

==== sorted

Unlike most map implementations, in HBase/BigTable the key/value pairs are kept in strict alphabetical order. That is to say that the row for the key "aaaaa" should be right next to the row with key "aaaab" and very far from the row with key "zzzzz".
Continuing our JSON example, the sorted version looks like this:

[source,json,subs="verbatim,attributes"]
----
	
{
  "1" : "x",
  "aaaaa" : "y",
  "aaaab" : "world",
  "xyz" : "hello",
  "zzzzz" : "woot"
}
----

Because these systems tend to be so huge and distributed, this sorting feature is actually very important. The spacial propinquity of rows with like keys ensures that when you must scan the table, the items of greatest interest to you are near each other.

This is important when choosing a row key convention. For example, consider a table whose keys are domain names. It makes the most sense to list them in reverse notation (so "com.jimbojw.www" rather than "www.jimbojw.com") so that rows about a subdomain will be near the parent domain row.

Continuing the domain example, the row for the domain "mail.jimbojw.com" would be right next to the row for "www.jimbojw.com" rather than say "mail.xyz.com" which would happen if the keys were regular domain notation.

It's important to note that the term "sorted" when applied to HBase/BigTable does not mean that "values" are sorted. There is no automatic indexing of anything other than the keys, just as it would be in a plain-old map implementation.

==== multidimensional

Up to this point, we haven't mentioned any concept of "columns", treating the "table" instead as a regular-old hash/map in concept. This is entirely intentional. The word "column" is another loaded word like "table" and "base" which carries the emotional baggage of years of RDBMS experience.

Instead, I find it easier to think about this like a multidimensional map - a map of maps if you will. Adding one dimension to our running JSON example gives us this:

[source,json,subs="verbatim,attributes"]
----
{
  "1" : {
    "A" : "x",
    "B" : "z"
  },
  "aaaaa" : {
    "A" : "y",
    "B" : "w"
  },
  "aaaab" : {
    "A" : "world",
    "B" : "ocean"
  },
  "xyz" : {
    "A" : "hello",
    "B" : "there"
  },
  "zzzzz" : {
    "A" : "woot",
    "B" : "1337"
  }
}
----

In the above example, you'll notice now that each key points to a map with exactly two keys: "A" and "B". From here forward, we'll refer to the top-level key/map pair as a "row". Also, in BigTable/HBase nomenclature, the "A" and "B" mappings would be called "Column Families".

A table's column families are specified when the table is created, and are difficult or impossible to modify later. It can also be expensive to add new column families, so it's a good idea to specify all the ones you'll need up front.

Fortunately, a column family may have any number of columns, denoted by a column "qualifier" or "label". Here's a subset of our JSON example again, this time with the column qualifier dimension built in:

[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "aaaaa" : {
    "A" : {
      "foo" : "y",
      "bar" : "d"
    },
    "B" : {
      "" : "w"
    }
  },
  "aaaab" : {
    "A" : {
      "foo" : "world",
      "bar" : "domination"
    },
    "B" : {
      "" : "ocean"
    }
  },
  // ...
}
----

Notice that in the two rows shown, the "A" column family has two columns: "foo" and "bar", and the "B" column family has just one column whose qualifier is the empty string ("").

When asking HBase/BigTable for data, you must provide the full column name in the form "family:qualifier". So for example, both rows in the above example have three columns: "A:foo", "A:bar" and "B:".

Note that although the column families are static, the columns themselves are not. Consider this expanded row:

[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "zzzzz" : {
    "A" : {
      "catch_phrase" : "woot",
    }
  }
}
----

In this case, the "zzzzz" row has exactly one column, "A:catch_phrase". Because each row may have any number of different columns, there's no built-in way to query for a list of all columns in all rows. To get that information, you'd have to do a full table scan. You can however query for a list of all column families since these are immutable (more-or-less).

The final dimension represented in HBase/BigTable is time. All data is versioned either using an integer timestamp (seconds since the epoch), or another integer of your choice. The client may specify the timestamp when inserting data.

Consider this updated example utilizing arbitrary integral timestamps:


[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "aaaaa" : {
    "A" : {
      "foo" : {
        15 : "y",
        4 : "m"
      },
      "bar" : {
        15 : "d",
      }
    },
    "B" : {
      "" : {
        6 : "w"
        3 : "o"
        1 : "w"
      }
    }
  },
  // ...
}
----

Each column family may have its own rules regarding how many versions of a given cell to keep (a cell is identified by its rowkey/column pair) In most cases, applications will simply ask for a given cell's data, without specifying a timestamp. In that common case, HBase/BigTable will return the most recent version (the one with the highest timestamp) since it stores these in reverse chronological order.

If an application asks for a given row at a given timestamp, HBase will return cell data where the timestamp is less than or equal to the one provided.

Using our imaginary HBase table, querying for the row/column of "aaaaa"/"A:foo" will return "y" while querying for the row/column/timestamp of "aaaaa"/"A:foo"/10 will return "m". Querying for a row/column/timestamp of "aaaaa"/"A:foo"/2 will return a null result.

==== sparse

The last keyword is sparse. As already mentioned, a given row can have any number of columns in each column family, or none at all. The other type of sparseness is row-based gaps, which merely means that there may be gaps between keys.

This, of course, makes perfect sense if you've been thinking about HBase/BigTable in the map-based terms of this article rather than perceived similar concepts in RDBMS's.

==== And that's about it

Well, I hope that helps you understand conceptually what the HBase data model feels like.

As always, I look forward to your thoughts, comments and suggestions.

 
 
== Quick start for installation

=== Installation 

Download the latest stable version from apache website.

Create an installation dir (the data storage is easily configurable)

My choice : 

 * centos 7
 * installation in my user home dir
 * configuration  of the data dir inside the _conf/hbase-site.xml_ file (see example bellow)
 * ensure java is installed, and JAVA_HOME is configured. export JAVA_HOME=/usr 

....
 <configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///opt/hbase/hbase</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/opt/hbase/zookeeper</value>
  </property>
</configuration>
....  

=== Start hbase

to start hbase simply run the startup script _bin/start-hbase.sh_
From this point you can access the administrative page : _lynx localhost:16010_ 
if you want to access it from an other server, you need to configure the firewall

....
sudo firewall-cmd --get-active-zones   #to list the zone where you have a firewall applicable
#need to configure it for all zone. Take care if you are in a dmz, or a secure area
sudo firewall-cmd --zone=public --add-port=16010/tcp --permanent
sudo firewall-cmd --reload
....

=== Connect to your local instance

simply run _./bin/hbase shell_


=== Create Table, put data, get results

To create a table : 
....
hbase(main):003:0> create 'test', 'cf'
0 row(s) in 1.4610 seconds

=> Hbase::Table - test
....

you can double check in the browser

image::hbase_createTable.png[hbase_createTable]

Now, we can add data, a get them 

....
hbase(main):002:0> put 'test', 'row1', 'cf:a', 'value1'
hbase(main):003:0> put 'test', 'row2', 'cf:b', 'value2'
hbase(main):004:0> put 'test', 'row3', 'cf:c', 'value3'

#to get the full content of the table
hbase(main):006:0> scan 'test'
ROW                               COLUMN+CELL
 row1                             column=cf:a, timestamp=1487598057519, value=value1
 row2                             column=cf:b, timestamp=1487598062099, value=value2
 row3                             column=cf:c, timestamp=1487598066972, value=value3
3 row(s) in 0.0220 seconds

#To get only one row
hbase(main):029:0* get 'test', 'row1'
COLUMN                            CELL
 cf:a                             timestamp=1487598057519, value=value1
1 row(s) in 0.0280 seconds
....

[TIP]
====
 before dropping a table, or alter it, you need to disable it.
 _disable 'test'_ 
====

=== Stop hbase

simply run _./bin/stop-hbase.sh_

== Over Hadoop, 


image::HbaseOverHadoop.png[HBase over hadoop]

if you want to move to hdfs :
....
<property>
  <name>hbase.rootdir</name>
  <value>hdfs://localhost:8020/hbase</value>
</property>
....

you'll see a new bunch of files, hbase will create them automatically.
Warning, if there's already an HBase directory, hbase will try to do a migration from the existing version.


== Now, let's go to an hbase cluster (we'll not discuss about hadoop cluster here ... )

=== First, separate zookeeper from the HBase master 

Update the _conf/hbase-site.xml_ with :

....
<property>
  <name>hbase.cluster.distributed</name>
  <value>true</value>
</property>
....


=== You need 3 hosts to do such setup


== Shell

=== shell from text file

You can enter HBase Shell commands into a text file, one command per line, and pass that file to the HBase Shell.

.Example command file

====
 create 'test', 'cf'
 list 'test'
 put 'test', 'row1', 'cf:a', 'value1'
 put 'test', 'row2', 'cf:b', 'value2'
 put 'test', 'row3', 'cf:c', 'value3'
 put 'test', 'row4', 'cf:d', 'value4'
 scan 'test'
 get 'test', 'row1'
 disable 'test'
 enable 'test'
====

.run command from file
====
 ./hbase shell ./sample_commands.txt
====


== DATA MODEL


.Table
    An HBase table consists of multiple rows.

.Row

    A row in HBase consists of a row key and one or more columns with values associated with them. Rows are sorted alphabetically by the row key as they are stored. For this reason, the design of the row key is very important. The goal is to store data in such a way that related rows are near each other. A common row key pattern is a website domain. If your row keys are domains, you should probably store them in reverse (org.apache.www, org.apache.mail, org.apache.jira). This way, all of the Apache domains are near each other in the table, rather than being spread out based on the first letter of the subdomain.
Column

    A column in HBase consists of a column family and a column qualifier, which are delimited by a : (colon) character.
Column Family

    Column families physically colocate a set of columns and their values, often for performance reasons. Each column family has a set of storage properties, such as whether its values should be cached in memory, how its data is compressed or its row keys are encoded, and others. Each row in a table has the same column families, though a given row might not store anything in a given column family.
Column Qualifier

    A column qualifier is added to a column family to provide the index for a given piece of data. Given a column family content, a column qualifier might be content:html, and another might be content:pdf. Though column families are fixed at table creation, column qualifiers are mutable and may differ greatly between rows.
Cell

    A cell is a combination of row, column family, and column qualifier, and contains a value and a timestamp, which represents the value’s version.
Timestamp

    A timestamp is written alongside each value, and is the identifier for a given version of a value. By default, the timestamp represents the time on the RegionServer when the data was written, but you can specify a different timestamp value when you put data into the cell.


== TIPS

[TIP]
====
 . ssh
 . dns
 . loopback entry
 . ntp
 . ulimit
====

=== _ssh_

HBase uses the Secure Shell (ssh) command and utilities extensively to communicate between cluster nodes. Each server in the cluster must be running ssh so that the Hadoop and HBase daemons can be managed. You must be able to connect to all nodes via SSH, including the local node, from the Master as well as any backup Master, using a shared key rather than a password. You can see the basic methodology for such a set-up in Linux or Unix systems at "Procedure: Configure Passwordless SSH Access". If your cluster nodes use OS X, see the section, SSH: Setting up Remote Desktop and Enabling Self-Login on the Hadoop wiki.

=== _DNS_

HBase uses the local hostname to self-report its IP address. Both forward and reverse DNS resolving must work in versions of HBase previous to 0.92.0. The hadoop-dns-checker tool can be used to verify DNS is working correctly on the cluster. The project README file provides detailed instructions on usage.

=== _Loopback IP_

Prior to hbase-0.96.0, HBase only used the IP address 127.0.0.1 to refer to localhost, and this could not be configured. See Loopback IP for more details.

=== _ NTP_

The clocks on cluster nodes should be synchronized. A small amount of variation is acceptable, but larger amounts of skew can cause erratic and unexpected behavior. Time synchronization is one of the first things to check if you see unexplained problems in your cluster. It is recommended that you run a Network Time Protocol (NTP) service, or another time-synchronization mechanism, on your cluster, and that all nodes look to the same service for time synchronization. See the Basic NTP Configuration at The Linux Documentation Project (TLDP) to set up NTP.===_Limits on Number of Files and Processes (ulimit)_

=== _ulimit_
Apache HBase is a database. It requires the ability to open a large number of files at once. Many Linux distributions limit the number of files a single user is allowed to open to 1024 (or 256 on older versions of OS X). You can check this limit on your servers by running the command ulimit -n when logged in as the user which runs HBase. See the Troubleshooting section for some of the problems you may experience if the limit is too low. You may also notice errors such as the following:
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901

It is recommended to raise the ulimit to at least 10,000, but more likely 10,240, because the value is usually expressed in multiples of 1024. Each ColumnFamily has at least one StoreFile, and possibly more than six StoreFiles if the region is under load. The number of open files required depends upon the number of ColumnFamilies and the number of regions. The following is a rough formula for calculating the potential number of open files on a RegionServer.
Calculate the Potential Number of Open Files

   (StoreFiles per ColumnFamily) x (regions per RegionServer)

For example, assuming that a schema had 3 ColumnFamilies per region with an average of 3 StoreFiles per ColumnFamily, and there are 100 regions per RegionServer, the JVM will open 3 * 3 * 100 = 900 file descriptors, not counting open JAR files, configuration files, and others. Opening a file does not take many resources, and the risk of allowing a user to open too many files is minimal.

Another related setting is the number of processes a user is allowed to run at once. In Linux and Unix, the number of processes is set using the ulimit -u command. This should not be confused with the nproc command, which controls the number of CPUs available to a given user. Under load, a ulimit -u that is too low can cause OutOfMemoryError exceptions. See Jack Levin's major HDFS issues thread on the hbase-users mailing list, from 2011.



