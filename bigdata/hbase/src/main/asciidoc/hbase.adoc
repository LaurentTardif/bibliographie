= HBase  
:toc:


== Introduction

=== What's HBASE

A non-relational (NoSQL) database that runs on top of HDFS

Apache HBase is an open source NoSQL database that provides real-time read/write access to those large datasets.

HBase scales linearly to handle huge data sets with billions of rows and millions of columns, and it easily combines data sources that use a wide variety of different structures and schemas. HBase is natively integrated with Hadoop and works seamlessly alongside other data access engines through YARN.
What HBase Does

Apache HBase provides random, real time access to your data in Hadoop. It was created for hosting very large tables, making it a great choice to store multi-structured or sparse data. Users can query HBase for a particular point in time, making “flashback” queries possible. These following characterisitcs make HBase a great choice for storing semi-structured data like log data and then providing that data very quickly to users or applications integrated with HBase.

 * Fault tolerant 	

	Replication across the data center
    Atomic and strongly consistent row-level operations
    High availability through automatic failover
    Automatic sharding and load balancings of tables
	
 * Scalling 

    Supports scaling out in coordination with Hadoop file system even on commodity hardware
	Very flexible on schema design/no fixed schema

 * Fast 	

    Near real time lookups
    In-memory caching via block cache and bloom filters
    Server side processing via filters and co-processors
	
 * Usable 	
 
    Data model accommodates wide range of use cases
    Metrics exports via File and Ganglia plugins
    Easy Java API as well as Thrift and REST gateway APIs
	Great for analytics in association with Hadoop MapReduce
	Can be integrated with Hive for SQL-like queries, which is better for DBAs who are more familiar with SQL queries
	
  * CONS	
	
	Single point of failure (when only one HMaster is used)
    No transaction support
    JOINs are handled in MapReduce layer rather than the database itself
    Indexed and sorted only on key, but RDBMS can be indexed on some arbitrary field
    No built-in authentication or permissions

=== Versions 

 * 2.0.0 (soon)
 * 1.3.0 (latest) : http://www-eu.apache.org/dist/hbase/1.3.0/hbase-1.3.0-bin.tar.gz
 * 1.2.5 (stable) : http://www-eu.apache.org/dist/hbase/stable/hbase-1.2.5-bin.tar.gz

=== Documentations

.From official references 

 * http://hbase.apache.org/book.html
 * Download link : http://www.apache.org/dyn/closer.cgi/hbase/

.From tutorials

 * http://www.guru99.com/hbase-tutorials.html
 * http://hortonworks.com/apache/hbase/
 * https://hbase.apache.org/book.html
 
.From Pro and cons
 
 * https://www.packtpub.com/mapt/book/big-data-and-business-intelligence/9781783985944/1/ch01lvl1sec16/hbase-pros-and-cons
 * http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part-3-pros-cons/
 * http://stackoverflow.com/questions/22542307/hbase-what-are-the-pros-and-cons-of-using-one-column-with-a-list-of-values-vs
 * http://www.slideshare.net/EdurekaIN/no-sql-databases-35591065

.Data Model 

 * http://jimbojw.com/#understanding%20hbase
 * http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/9353-login1210_khurana.pdf
 
.Performance

 * https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines
 
.Tips 

 * https://www.dynamicyield.com/2015/05/apache-hbase-for-the-win-2/
 * http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/
 * http://www.techsquids.com/bd/hbase-scan-filters-tips-tricks/
 * http://lecluster.delaurent.com/hbase-tips-tricks/
 * https://intellipaat.com/interview-question/hbase-interview-questions/
 * https://dzone.com/articles/handling-big-data-hbase-part-5
 * http://www.slideshare.net/lhofhansl/h-base-tuninghbasecon2015ok

 
 
== Hbase basics and Key concepts

 
=== Understanding HBASE and BigTable 


[NOTE]
.From Jim R.Wilson, May 2008
-----

The hardest part about learning HBase (the open source implementation of Google's BigTable), is just wrapping your mind around the concept of what it actually is.

I find it rather unfortunate that these two great systems contain the words table and base in their names, which tend to cause confusion among RDBMS indoctrinated individuals (like myself).

This article aims to describe these distributed data storage systems from a conceptual standpoint. After reading it, you should be better able to make an educated decision regarding when you might want to use HBase vs when you'd be better off with a "traditional" database.
it's all in the terminology

Fortunately, Google's BigTable Paper clearly explains what BigTable actually is. Here is the first sentence of the "Data Model" section:

    A Bigtable is a sparse, distributed, persistent multidimensional sorted map.

Note: At this juncture I like to give readers the opportunity to collect any brain matter which may have left their skulls upon reading that last line.

The BigTable paper continues, explaining that:

    The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes.

Along those lines, the HBaseArchitecture page of the Hadoop wiki posits that:

    HBase uses a data model very similar to that of Bigtable. Users store data rows in labelled tables. A data row has a sortable key and an arbitrary number of columns. The table is stored sparsely, so that rows in the same table can have crazily-varying columns, if the user likes.

Although all of that may seem rather cryptic, it makes sense once you break it down a word at a time. I like to discuss them in this sequence: map, persistent, distributed, sorted, multidimensional, and sparse.

Rather than trying to picture a complete system all at once, I find it easier to build up a mental framework piecemeal, to ease into it...
-----


=== map

At its core, HBase/BigTable is a map. Depending on your programming language background, you may be more familiar with the terms associative array (PHP), dictionary (Python), Hash (Ruby), or Object (JavaScript).

From the wikipedia article, a map is "an abstract data type composed of a collection of keys and a collection of values, where each key is associated with one value."

Using JavaScript Object Notation, here's an example of a simple map where all the values are just strings:

[source,json,subs="verbatim,attributes"]
----
{
  "zzzzz" : "woot",
  "xyz" : "hello",
  "aaaab" : "world",
  "1" : "x",
  "aaaaa" : "y"
}
----

=== persistent

Persistence merely means that the data you put in this special map "persists" after the program that created or accessed it is finished. This is no different in concept than any other kind of persistent storage such as a file on a filesystem. Moving along...

=== distributed

HBase and BigTable are built upon distributed filesystems so that the underlying file storage can be spread out among an array of independent machines.

HBase sits atop either Hadoop's Distributed File System (HDFS) or Amazon's Simple Storage Service (S3), while a BigTable makes use of the Google File System (GFS).

Data is replicated across a number of participating nodes in an analogous manner to how data is striped across discs in a RAID system.

For the purpose of this article, we don't really care which distributed filesystem implementation is being used. The important thing to understand is that it is distributed, which provides a layer of protection against, say, a node within the cluster failing.

=== sorted

Unlike most map implementations, in HBase/BigTable the key/value pairs are kept in strict alphabetical order. That is to say that the row for the key "aaaaa" should be right next to the row with key "aaaab" and very far from the row with key "zzzzz".
Continuing our JSON example, the sorted version looks like this:

[source,json,subs="verbatim,attributes"]
----
	
{
  "1" : "x",
  "aaaaa" : "y",
  "aaaab" : "world",
  "xyz" : "hello",
  "zzzzz" : "woot"
}
----

Because these systems tend to be so huge and distributed, this sorting feature is actually very important. The spacial propinquity of rows with like keys ensures that when you must scan the table, the items of greatest interest to you are near each other.

This is important when choosing a row key convention. For example, consider a table whose keys are domain names. It makes the most sense to list them in reverse notation (so "com.jimbojw.www" rather than "www.jimbojw.com") so that rows about a subdomain will be near the parent domain row.

Continuing the domain example, the row for the domain "mail.jimbojw.com" would be right next to the row for "www.jimbojw.com" rather than say "mail.xyz.com" which would happen if the keys were regular domain notation.

It's important to note that the term "sorted" when applied to HBase/BigTable does not mean that "values" are sorted. There is no automatic indexing of anything other than the keys, just as it would be in a plain-old map implementation.

=== multidimensional

Up to this point, we haven't mentioned any concept of "columns", treating the "table" instead as a regular-old hash/map in concept. This is entirely intentional. The word "column" is another loaded word like "table" and "base" which carries the emotional baggage of years of RDBMS experience.

Instead, I find it easier to think about this like a multidimensional map - a map of maps if you will. Adding one dimension to our running JSON example gives us this:

[source,json,subs="verbatim,attributes"]
----
{
  "1" : {
    "A" : "x",
    "B" : "z"
  },
  "aaaaa" : {
    "A" : "y",
    "B" : "w"
  },
  "aaaab" : {
    "A" : "world",
    "B" : "ocean"
  },
  "xyz" : {
    "A" : "hello",
    "B" : "there"
  },
  "zzzzz" : {
    "A" : "woot",
    "B" : "1337"
  }
}
----

In the above example, you'll notice now that each key points to a map with exactly two keys: "A" and "B". From here forward, we'll refer to the top-level key/map pair as a "row". Also, in BigTable/HBase nomenclature, the "A" and "B" mappings would be called "Column Families".

A table's column families are specified when the table is created, and are difficult or impossible to modify later. It can also be expensive to add new column families, so it's a good idea to specify all the ones you'll need up front.

Fortunately, a column family may have any number of columns, denoted by a column "qualifier" or "label". Here's a subset of our JSON example again, this time with the column qualifier dimension built in:

[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "aaaaa" : {
    "A" : {
      "foo" : "y",
      "bar" : "d"
    },
    "B" : {
      "" : "w"
    }
  },
  "aaaab" : {
    "A" : {
      "foo" : "world",
      "bar" : "domination"
    },
    "B" : {
      "" : "ocean"
    }
  },
  // ...
}
----

Notice that in the two rows shown, the "A" column family has two columns: "foo" and "bar", and the "B" column family has just one column whose qualifier is the empty string ("").

When asking HBase/BigTable for data, you must provide the full column name in the form "family:qualifier". So for example, both rows in the above example have three columns: "A:foo", "A:bar" and "B:".

Note that although the column families are static, the columns themselves are not. Consider this expanded row:

[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "zzzzz" : {
    "A" : {
      "catch_phrase" : "woot",
    }
  }
}
----

In this case, the "zzzzz" row has exactly one column, "A:catch_phrase". Because each row may have any number of different columns, there's no built-in way to query for a list of all columns in all rows. To get that information, you'd have to do a full table scan. You can however query for a list of all column families since these are immutable (more-or-less).

The final dimension represented in HBase/BigTable is time. All data is versioned either using an integer timestamp (seconds since the epoch), or another integer of your choice. The client may specify the timestamp when inserting data.

Consider this updated example utilizing arbitrary integral timestamps:


[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "aaaaa" : {
    "A" : {
      "foo" : {
        15 : "y",
        4 : "m"
      },
      "bar" : {
        15 : "d",
      }
    },
    "B" : {
      "" : {
        6 : "w"
        3 : "o"
        1 : "w"
      }
    }
  },
  // ...
}
----

Each column family may have its own rules regarding how many versions of a given cell to keep (a cell is identified by its rowkey/column pair) In most cases, applications will simply ask for a given cell's data, without specifying a timestamp. In that common case, HBase/BigTable will return the most recent version (the one with the highest timestamp) since it stores these in reverse chronological order.

If an application asks for a given row at a given timestamp, HBase will return cell data where the timestamp is less than or equal to the one provided.

Using our imaginary HBase table, querying for the row/column of "aaaaa"/"A:foo" will return "y" while querying for the row/column/timestamp of "aaaaa"/"A:foo"/10 will return "m". Querying for a row/column/timestamp of "aaaaa"/"A:foo"/2 will return a null result.

=== sparse

The last keyword is sparse. As already mentioned, a given row can have any number of columns in each column family, or none at all. The other type of sparseness is row-based gaps, which merely means that there may be gaps between keys.

This, of course, makes perfect sense if you've been thinking about HBase/BigTable in the map-based terms of this article rather than perceived similar concepts in RDBMS's.

=== And that's about it

Well, I hope that helps you understand conceptually what the HBase data model feels like.

As always, I look forward to your thoughts, comments and suggestions.

 
 
= Installation

== Quick start for Installation 

Download the latest stable version from apache website.

Create an installation dir (the data storage is easily configurable)

My choice : 

 * centos 7
 * installation in my user home dir
 * ensure java is installed, and JAVA_HOME is configured. export JAVA_HOME=/usr 
 * configuration  of the data dir inside the _conf/hbase-site.xml_ file (see example bellow)

.conf/hbase-site.xml
....
 <configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file://home/cnam/data/hbase</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/home/cnam/data/zookeeper</value>
  </property>
</configuration>
....  

=== Start hbase

To start hbase simply run the startup script _bin/start-hbase.sh_
From this point you can access the administrative page : _lynx localhost:16010_ 
if you want to access it from an other server, you need to configure the firewall

....
sudo firewall-cmd --get-active-zones   #to list the zone where you have a firewall applicable

#need to configure it for all zone. Take care if you are in a dmz, or a secure area
sudo firewall-cmd --zone=public --add-port=16010/tcp --permanent
sudo firewall-cmd --reload
....

In case you are using virtualbox or a similar tool, you need also to map the ports to your VM.

=== Result 


image::HBaseDeploiement\Diapositive2.png[REsult]


=== Connect to your local instance

simply run _./bin/hbase shell_


=== Create Table, put data, get results

To create a table : 
....
hbase(main):003:0> create 'test', 'cf'
0 row(s) in 1.4610 seconds

=> Hbase::Table - test
....

you can double check in the browser

image::hbase_createTable.png[hbase_createTable]

Now, we can add data, a get them 

....
hbase(main):002:0> put 'test', 'row1', 'cf:a', 'value1'
hbase(main):003:0> put 'test', 'row2', 'cf:b', 'value2'
hbase(main):004:0> put 'test', 'row3', 'cf:c', 'value3'

#to get the full content of the table
hbase(main):006:0> scan 'test'
ROW                               COLUMN+CELL
 row1                             column=cf:a, timestamp=1487598057519, value=value1
 row2                             column=cf:b, timestamp=1487598062099, value=value2
 row3                             column=cf:c, timestamp=1487598066972, value=value3
3 row(s) in 0.0220 seconds

#To get only one row
hbase(main):029:0* get 'test', 'row1'
COLUMN                            CELL
 cf:a                             timestamp=1487598057519, value=value1
1 row(s) in 0.0280 seconds
....

[TIP]
====
 before dropping a table, or alter it, you need to disable it.
 _disable 'test'_ 
====

=== basics commands

 * _list_  will list all tables
 * _describe '<TableName>'_ will describe the table 
 

=== Stop hbase

simply run _./bin/stop-hbase.sh_



== Over Hadoop, 


if you want to move to hdfs, first, you'll have to install Hadoop, and then tell HBase to use it :
....
<property>
  <name>hbase.rootdir</name>
  <value>hdfs://localhost:8020/hbase</value>
</property>
....

you'll see a new bunch of files, hbase will create them automatically.
Warning, if there's already an HBase directory, hbase will try to do a migration from the existing version.


image::HbaseOverHadoop.png[HBase over hadoop]



image::HBaseDeploiement\Diapositive3.png[REsult]

image::HBaseDeploiement\Diapositive4.png[REsult]


== Pseudo HBase cluster

[NOTE]
 we'll not discuss about hadoop cluster here ... 

=== First, separate zookeeper from the HBase master 

Update the _conf/hbase-site.xml_ with :

....
<property>
  <name>hbase.cluster.distributed</name>
  <value>true</value>
</property>
....

=== Result 


image::HBaseDeploiement\Diapositive5.png[REsult]


=== How to check it's working 

==== do you have jps (Java Virtual Machine Process Status Tool)

it's not installed by default with openJDK, you need to install the devel modules

 [admin@localhost hbase]$ sudo yum list *java*devel*
 Modules complémentaires chargés : fastestmirror, langpacks
 Loading mirror speeds from cached hostfile
  * base: miroir.univ-paris13.fr
  * extras: mirrors.ircam.fr
  * updates: miroir.univ-paris13.fr
 Paquets disponibles
 java-1.6.0-openjdk-devel.x86_64                     1:1.6.0.41-1.13.13.1.el7_3                updates
 java-1.7.0-openjdk-devel.x86_64                     1:1.7.0.131-2.6.9.0.el7_3                 updates
 java-1.8.0-openjdk-devel.i686                       1:1.8.0.121-0.b13.el7_3                   updates
 java-1.8.0-openjdk-devel.x86_64                     1:1.8.0.121-0.b13.el7_3                   updates
 java-1.8.0-openjdk-devel-debug.i686                 1:1.8.0.121-0.b13.el7_3                   updates
 java-1.8.0-openjdk-devel-debug.x86_64               1:1.8.0.121-0.b13.el7_3                   updates
 libdb-java-devel.i686                               5.3.21-19.el7                             base   
 libdb-java-devel.x86_64                             5.3.21-19.el7                             base   
 libguestfs-java-devel.x86_64                        1:1.32.7-3.el7.centos.2                   updates
 libvirt-java-devel.noarch                           0.4.9-4.el7                               base   

and then

 yum install java-1.8.0-openjdk-devel.x86_64
 

==== run jps command

 [admin@localhost hbase]$ jps
 10066 SecondaryNameNode
 10619 HMaster
 9741 NameNode
 9885 DataNode
 10541 HQuorumPeer
 10718 HRegionServer
 12494 Jps

=== Real HBASE cluster

You need 4 hosts to do such setup

== Configuration tips

A few configuration recommendations include disabling auto-compaction (by default it happens every 24 hours from the time you start HBase) and schedule it to run every day at an off-peak time. You should also configure compression (such as LZO) and explicitly put the correctly configured HBase conf directory in your CLASSPATH.

== Documentation 


== Result

image::HBaseDeploiement\Diapositive7.png[REsult]


image::HBaseDeploiement\Diapositive6.png[REsult with port number]



== Installation TIPS

[TIP]
====
 . ssh
 . dns
 . loopback entry
 . ntp
 . ulimit
====

=== _ssh_

HBase uses the Secure Shell (ssh) command and utilities extensively to communicate between cluster nodes. Each server in the cluster must be running ssh so that the Hadoop and HBase daemons can be managed. You must be able to connect to all nodes via SSH, including the local node, from the Master as well as any backup Master, using a shared key rather than a password. You can see the basic methodology for such a set-up in Linux or Unix systems at "Procedure: Configure Passwordless SSH Access". If your cluster nodes use OS X, see the section, SSH: Setting up Remote Desktop and Enabling Self-Login on the Hadoop wiki.

=== _DNS_

HBase uses the local hostname to self-report its IP address. Both forward and reverse DNS resolving must work in versions of HBase previous to 0.92.0. The hadoop-dns-checker tool can be used to verify DNS is working correctly on the cluster. The project README file provides detailed instructions on usage.

=== _Loopback IP_

Prior to hbase-0.96.0, HBase only used the IP address 127.0.0.1 to refer to localhost, and this could not be configured. See Loopback IP for more details.

=== _ NTP_

The clocks on cluster nodes should be synchronized. A small amount of variation is acceptable, but larger amounts of skew can cause erratic and unexpected behavior. Time synchronization is one of the first things to check if you see unexplained problems in your cluster. It is recommended that you run a Network Time Protocol (NTP) service, or another time-synchronization mechanism, on your cluster, and that all nodes look to the same service for time synchronization. See the Basic NTP Configuration at The Linux Documentation Project (TLDP) to set up NTP.===_Limits on Number of Files and Processes (ulimit)_

=== _ulimit_
Apache HBase is a database. It requires the ability to open a large number of files at once. Many Linux distributions limit the number of files a single user is allowed to open to 1024 (or 256 on older versions of OS X). You can check this limit on your servers by running the command ulimit -n when logged in as the user which runs HBase. See the Troubleshooting section for some of the problems you may experience if the limit is too low. You may also notice errors such as the following:
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901

It is recommended to raise the ulimit to at least 10,000, but more likely 10,240, because the value is usually expressed in multiples of 1024. Each ColumnFamily has at least one StoreFile, and possibly more than six StoreFiles if the region is under load. The number of open files required depends upon the number of ColumnFamilies and the number of regions. The following is a rough formula for calculating the potential number of open files on a RegionServer.
Calculate the Potential Number of Open Files

   (StoreFiles per ColumnFamily) x (regions per RegionServer)

For example, assuming that a schema had 3 ColumnFamilies per region with an average of 3 StoreFiles per ColumnFamily, and there are 100 regions per RegionServer, the JVM will open 3 * 3 * 100 = 900 file descriptors, not counting open JAR files, configuration files, and others. Opening a file does not take many resources, and the risk of allowing a user to open too many files is minimal.

Another related setting is the number of processes a user is allowed to run at once. In Linux and Unix, the number of processes is set using the ulimit -u command. This should not be confused with the nproc command, which controls the number of CPUs available to a given user. Under load, a ulimit -u that is too low can cause OutOfMemoryError exceptions. See Jack Levin's major HDFS issues thread on the hbase-users mailing list, from 2011.

== Ports 

 * http://yourhost:16010/master-status for the Master Server
 * http://yourhost:9095/thrift.jsp for the thrift UI (if activated)
 * http://yourhost:8085/rest.jsp for the REST server UI (if activated)
 * http://yourhost:16010/zk.jsp for the embedded Zookeeper


== Metrics 

ref : http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/

You should also keep the number of regions to a reasonable number based on memstore size and amount of RAM and the RegionServer JVM should be limited to 12GB of java heap to minimize long GC pauses. 

For example a machine with 36GB of RAM that is also running a DataNode daemon could handle approximately 100 regions with active writes and a memstore of 48MB each.

That allows enough headroom for DataNode and RegionServer memory requirements, Linux file buffer space and a reasonable flush size for each RegionServer.


== Monitoring

you can use lot of tooling to do it. The keys in hbase cluster is to:
 * be able to indentify hot-spot among the HRegionServer
 * be able to identify some read of write pattern having generating hotspot
 
Example of application : 
 * introscope
 * appdynamique 

=== Nagios [See Installing Nagios]

A good starting set of plugins can be found at : git clone https://github.com/harisekhon/nagios-plugins

check_hbase_*.pl - various HBase monitoring utilities using Thrift + Stargate APIs, checking Masters / Backup Masters, RegionServers, table availability (exists, is enabled, and has minimum number of column families), number of expected table regions, unassigned table regions, regions stuck in transition, region count balance across RegionServers, compaction in progress (by table and by regionserver), number of regions in transition, longest current region migration time, hbck status and any inconsistencies, cell content vs optional regex + thresholds, table write and read back of unique generated values with write/read/delete latency checks against all detected column families, table write spray and read back of unique values across all regions for all column families with write/read/delete latency checks, gather metrics



== Let's use a bit our installation


=== Create a musicband table

 create 'musicband', 'informations','show'

 hbase(main):004:0> describe 'musicband'
 Table musicband is ENABLED
 musicband
 COLUMN FAMILIES DESCRIPTION
 {NAME => 'informations', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}
 {NAME => 'show', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}
 2 row(s) in 0.1130 seconds

 
=== Create some data

 hbase(main):006:0>  put 'musicband', 'The core' , 'informations:Year', '1999'
 hbase(main):007:0>  put 'musicband', 'The core' , 'informations:Style', 'Jazz'
 hbase(main):008:0>  put 'musicband', 'The core' , 'informations:Country', 'Norway'
 
 hbase(main):009:0> scan 'musicband' , {COLUMN => 'informations'}
 ROW                                COLUMN+CELL
  The core                          column=informations:Country, timestamp=1491408111337, value=Norway
  The core                          column=informations:Lead, timestamp=1491407996565, value=Espen Aalberg
  The core                          column=informations:Style, timestamp=1491408102049, value=Jazz
  The core                          column=informations:Year, timestamp=1491408074355, value=1999
  U2                                column=informations:Name, timestamp=1491396362728, value=U2
  U2                                column=informations:Singer, timestamp=1491396400135, value=Bono
  U2                                column=informations:Style, timestamp=1491396460243, value=Rock
  U2                                column=informations:Year, timestamp=1491396452455, value=1976
 2 row(s) in 0.0920 seconds
 
 
=== populate some data

Let's do a linear massive import in order to see what's happening.

   for i in `seq 1 10` ; do echo put "'musicband'", "'U2-Paris-1980-$i'", "'show:Number'", "'$i'" >> insertNotSoMassive.txt; done
  ./hbase shell < insertNotSoMassive.txt

=== populate far more and observe

Let's do a linear massive import in order to see what's happening.

   for i in `seq 10 1000000` ; do echo put "'musicband'", "'U2-Paris-1980-$i'", "'show:Number'", "'$i'" >> insertMassive.txt; done
  ./hbase shell < insertMassive.txt

==== what's happening on write side

image::PerformanceDisk.png[HBase on disk]

.Automatic split happening on regular basis
image::PerformanceDisk_Compact.png[HBase on disk with automatic compaction ]

.Region after the split
image::HBase_RegionAfterSplit.png[Split result]

=== doing some queries

.scan the information column 
 hbase(main):027:0> scan 'musicband' , {COLUMNS=>'informations',LIMIT=>2}
 ROW                                COLUMN+CELL
  U2                                column=informations:Name, timestamp=1491396362728, value=U2
  U2                                column=informations:Singer, timestamp=1491396400135, value=Bono
  U2                                column=informations:Style, timestamp=1491396460243, value=Rock
  U2                                column=informations:Year, timestamp=1491396452455, value=1976
 1 row(s) in 0.0940 seconds

.scan the show column (the one with one million entries)
 hbase(main):028:0> scan 'musicband' , {COLUMNS=>'show',LIMIT=>2}
 ROW                                COLUMN+CELL
  Paris-1980-1                      column=show:Number, timestamp=1491397656171, value=1
  Paris-1980-10                     column=show:Number, timestamp=1491397656512, value=10
 2 row(s) in 0.0310 seconds

.start at row 300000 ...

 hbase(main):012:0> scan 'musicband' , {COLUMNS=>'show',STARTROW => 'Paris-1980-300000', LIMIT=>2}
 ROW                                COLUMN+CELL
  Paris-1980-4                      column=show:Number, timestamp=1491397656329, value=4
  Paris-1980-5                      column=show:Number, timestamp=1491397656360, value=5
 2 row(s) in 0.0200 seconds

[TIP]
Take care of the alphabetical order

=== conclusion

= Hbase 

[Note] 
 a detailled blog can be found at : https://www.edureka.co/blog/hbase-architecture/


== Introduction to hbase architecture

HBase has three major components i.e., HMaster Server, HBase Region Server  and Zookeeper.

image::HBase-Architecture.png[Hbase Architecture]

The HMaster in the HBase is responsible for

 * Performing Administration
 * Managing and Monitoring the Cluster
 * Assigning Regions to the Region Servers
 * Controlling the Load Balancing and Failover

On the other hand, the HRegionServer perform the following work

 * Hosting and managing Regions
 * Splitting the Regions automatically
 * Handling the read/write requests
 * Communicating with the Clients directly

Each Region Server contains a Write-Ahead Log (called HLog) and multiple Regions. Each Region in turn is made up of a MemStore and multiple StoreFiles (HFile). The data lives in these StoreFiles in the form of Column Families (explained below). The MemStore holds in-memory modifications to the Store (data).

The mapping of Regions to Region Server is kept in a system table called .META. When trying to read or write data from HBase, the clients read the required Region information from the .META table and directly communicate with the appropriate Region Server. Each Region is identified by the start key (inclusive) and the end key (exclusive)

=== Region server

A region contains all the rows between the start key and the end key assigned to that region. HBase tables can be divided into a number of regions in such a way that all the columns of a column family is stored in one region. Each region contains the rows in a sorted order.

Many regions are assigned to a Region Server, which is responsible for handling, managing, executing reads and writes operations on that set of regions.

So, concluding in a simpler way:

 * A table can be divided into a number of regions. A Region is a sorted range of rows storing data between a start key and an end key.
 * A Region has a default size of 256MB which can be configured according to the need.
 * A Group of regions is served to the clients by a Region Server.
 * A Region Server can serve approximately 1000 regions to the client.

A Region Server maintains various regions running on the top of HDFS. Components of a Region Server are:

 * WAL: As you can conclude from the above image, Write Ahead Log (WAL) is a file attached to every Region Server inside the distributed environment. The WAL stores the new data that hasn’t been persisted or committed to the permanent storage. It is used in case of failure to recover the data sets.
 * Block Cache: From the above image, it is clearly visible that Block Cache resides in the top of Region Server. It stores the frequently read data in the memory. If the data in BlockCache is least recently used, then that data is removed from BlockCache.
 * MemStore: It is the write cache. It stores all the incoming data before committing it to the disk or permanent memory. There is one MemStore for each column family in a region. As you can see in the image, there are multiple MemStores for a region because each region contains multiple column families. The data is sorted in lexicographical order before committing it to the disk. 
 * HFile: From the above figure you can see HFile is stored on HDFS. Thus it stores the actual cells on the disk. MemStore commits the data to HFile when the size of MemStore exceeds.

image::RegionServer.png[Region server]
 
=== Master

HBase Components - HBase Architecture - Edureka

 * HBase HMaster performs DDL operations (create and delete tables) and assigns regions to the Region servers as you can see in the above image.
 * It coordinates and manages the Region Server (similar as NameNode manages DataNode in HDFS).
 * It assigns regions to the Region Servers on startup and re-assigns regions to Region Servers during recovery and load balancing.
 * It monitors all the Region Server’s instances in the cluster (with the help of Zookeeper) and performs recovery activities whenever any Region Server is down.
 * It provides an interface for creating, deleting and updating tables.

HBase has a distributed and huge environment where HMaster alone is not sufficient to manage everything. So, you would be wondering what helps HMaster to manage this huge environment? That’s where ZooKeeper comes into the picture. After we understood how HMaster manages HBase environment, we will understand how Zookeeper helps HMaster in managing the environment. 

image::HBase-Master.png[HMaster]


=== Zookeeper – The Coordinator

 * Zookeeper acts like a coordinator inside HBase distributed environment. It helps in maintaining server state inside the cluster by communicating through sessions.
 * Every Region Server along with HMaster Server sends continuous heartbeat at regular interval to Zookeeper and it checks which server is alive and available as mentioned in above image. It also provides server failure notifications so that, recovery measures can be executed.
 * Referring from the above image you can see, there is an inactive server, which acts as a backup for active server. If the active server fails, it comes for the rescue.
 * The active HMaster sends heartbeats to the Zookeeper while the inactive HMaster listens for the notification send by active HMaster. If the active HMaster fails to send a heartbeat the session is deleted and the inactive HMaster becomes active.
 * While if a Region Server fails to send a heartbeat, the session is expired and all listeners are notified about it. Then HMaster performs suitable recovery actions which we will discuss later in this blog.
 * Zookeeper also maintains the .META Server’s path, which helps any client in searching for any region. The Client first has to check with .META Server in which Region Server a region belongs, and it gets the path of that Region Server. 

image::ZooKeeper.png[Zookepper]

=== Meta table

image::Meta-Table-Hbase.png[Hbase meta table]
 
The META table is a special HBase catalog table. It maintains a list of all the Regions Servers in the HBase storage system, as you can see in the above image.
Looking at the figure you can see, .META file maintains the table in form of keys and values. Key represents the start key of the region and its id whereas the value contains the path of the Region Server.

== Read and write operations

=== Write mechanism

he write mechanism goes through the following process sequentially (refer to the above image): 

 * Step 1: Whenever the client has a write request, the client writes the data to the WAL (Write Ahead Log). 
    The edits are then appended at the end of the WAL file.
    This WAL file is maintained in every Region Server and Region Server uses it to recover data which is not committed to the disk.
 * Step 2: Once data is written to the WAL, then it is copied to the MemStore.
 * Step 3: Once the data is placed in MemStore, then the client receives the acknowledgment.
 * Step 4: When the MemStore reaches the threshold, it dumps or commits the data into a HFile.

image::HBase-Write.png[Write in Hbase]

.HBase Write Mechanism- MemStore

 * The MemStore always updates the data stored in it, in a lexicographical order (sequentially in a dictionary manner) as sorted KeyValues. There is one MemStore for each column family, and thus the updates are stored in a sorted manner for each column family. 
 * When the MemStore reaches the threshold, it dumps all the data into a new HFile in a sorted manner. This HFile is stored in HDFS. HBase contains multiple HFiles for each Column Family.
 * Over time, the number of HFile grows as MemStore dumps the data.
 * MemStore also saves the last written sequence number, so Master Server and MemStore both knows, that what is committed so far and where to start from. When region starts up, the last sequence number is read, and from that number, new edits start.

As I discussed several times, that HFile is the main persistent storage in an HBase architecture. At last, all the data is committed to HFile which is the permanent storage of HBase. Hence, let us look at the properties of HFile which makes it faster for search while reading and writing.

.HBase Write Mechanism- HFile

 * The writes are placed sequentially on the disk. Therefore, the movement of the disk’s read-write head is very less. This makes write and search mechanism very fast.
 * The HFile indexes are loaded in memory whenever an HFile is opened. This helps in finding a record in a single seek. 
 * The trailer is a pointer which points to the HFile’s meta block . It is written at the end of the committed file. It contains information about timestamp and bloom filters.
 * Bloom Filter helps in searching key value pairs, it skips the file which does not contain the required rowkey. Timestamp also helps in searching a version of the file, it helps in skipping the data.

=== Read Mechanism

As discussed in our search mechanism, first the client retrieves the location of the Region Server from .META Server if the client does not have it in its cache memory. Then it goes through the sequential steps as follows: 

 * For reading the data, the scanner first looks for the Row cell in Block cache. Here all the recently read key value pairs are stored.
 * If Scanner fails to find the required result, it moves to the MemStore, as we know this is the write cache memory. There, it searches for the most recently written files, which has not been dumped yet in HFile.
 * At last, it will use bloom filters and block cache to load the data from HFile.



== Administration 

=== Compaction

HBase combines HFiles to reduce the storage and reduce the number of disk seeks needed for a read. This process is called compaction. Compaction chooses some HFiles from a region and combines them. There are two types of compaction as you can see in the above image.

 * Minor Compaction: HBase automatically picks smaller HFiles and recommits them to bigger HFiles as shown in the above image. This is called Minor Compaction. It performs merge sort for committing smaller HFiles to bigger HFiles. This helps in storage space optimization. 
 * Major Compaction: As illustrated in the above image, in Major compaction, HBase merges and recommits the smaller HFiles of a region to a new HFile. In this process, the same column families are placed together in the new HFile. It drops deleted and expired cell in this process. It increases read performance.

But during this process, input-output disks and network traffic might get congested. This is known as write amplification. So, it is generally scheduled during low peak load timings.

image::Compaction-in-HBase.png[Compaction in Hbase]

=== Region split 

Whenever a region becomes large, it is divided into two child regions, as shown in the above figure. Each region represents exactly a half of the parent region. Then this split is reported to the HMaster. This is handled by the same Region Server until the HMaster allocates them to a new Region Server for load balancing.

image::HBase-Region-Split.png[Region split]

=== Crash and recovery


 * Whenever a Region Server fails, ZooKeeper notifies to the HMaster about the failure.
 * Then HMaster distributes and allocates the regions of crashed Region Server to many active Region Servers. To recover the data of the MemStore of the failed Region Server, the HMaster distributes the WAL to all the Region Servers.
 * Each Region Server re-executes the WAL to build the MemStore for that failed region’s column family.
 * The data is written in chronological order (in a timely order) in WAL. Therefore, Re-executing that WAL means making all the change that were made and stored in the MemStore file.
 * So, after all the Region Servers executes the WAL, the MemStore data for all column family is recovered.

== Toolings

==== YCSB (Yahoo! Cloud Serving Benchmark )

https://github.com/brianfrankcooper/YCSB/wiki



= DATA MODEL

== Concepts

TODO: http://jimbojw.com/#understanding%20hbase

.Namespace 

	A namespace is a logical grouping of tables analogous to a database in relation database systems. This abstraction lays the groundwork for upcoming multi-tenancy related features:
 * Quota Management (HBASE-8410) - Restrict the amount of resources (i.e. regions, tables) a namespace can consume.
 * Namespace Security Administration (HBASE-9206) - Provide another level of security administration for tenants.
 * Region server groups (HBASE-6721) - A namespace/table can be pinned onto a subset of RegionServers thus guaranteeing a course level of isolation.

.Table

    An HBase table consists of multiple rows.

.Row

    A row in HBase consists of a row key and one or more columns with values associated with them. Rows are sorted alphabetically by the row key as they are stored. For this reason, the design of the row key is very important. The goal is to store data in such a way that related rows are near each other. A common row key pattern is a website domain. If your row keys are domains, you should probably store them in reverse (org.apache.www, org.apache.mail, org.apache.jira). This way, all of the Apache domains are near each other in the table, rather than being spread out based on the first letter of the subdomain.
Column

.Rowkey 

	Row keys are uninterpreted bytes. Rows are lexicographically sorted with the lowest order appearing first in a table. The empty byte array is used to denote both the start and end of a tables' namespace.

.Column

    A column in HBase consists of a column family and a column qualifier, which are delimited by a : (colon) character.
Column Family

.ColumnFamily

    Columns in Apache HBase are grouped into column families. All column members of a column family have the same prefix. For example, the columns courses:history and courses:math are both members of the courses column family. The colon character (:) delimits the column family from the column family qualifier. The column family prefix must be composed of printable characters. The qualifying tail, the column family qualifier, can be made of any arbitrary bytes. Column families must be declared up front at schema definition time whereas columns do not need to be defined at schema time but can be conjured on the fly while the table is up and running.
Physically, all column family members are stored together on the filesystem. Because tunings and storage specifications are done at the column family level, it is advised that all column family members have the same general access pattern and size characteristics.
Column families physically colocate a set of columns and their values, often for performance reasons. Each column family has a set of storage properties, such as whether its values should be cached in memory, how its data is compressed or its row keys are encoded, and others. Each row in a table has the same column families, though a given row might not store anything in a given column family.
Column Qualifier

.ColumnQualifier

    A column qualifier is added to a column family to provide the index for a given piece of data. Given a column family content, a column qualifier might be content:html, and another might be content:pdf. Though column families are fixed at table creation, column qualifiers are mutable and may differ greatly between rows.
Cell

.Cell

    A cell is a combination of row, column family, and column qualifier, and contains a value and a timestamp, which represents the value’s version.
Timestamp. The HBase version dimension is stored in decreasing order, so that when reading from a store file, the most recent values are found first.


.Timestamp

    A timestamp is written alongside each value, and is the identifier for a given version of a value. By default, the timestamp represents the time on the RegionServer when the data was written, but you can specify a different timestamp value when you put data into the cell.

	
[TIP]
====
The maximum number of versions to store for a given column is part of the column schema and is specified at table creation, or via an alter command, via HColumnDescriptor.DEFAULT_VERSIONS. 
==== 

[TIP]
.Modify the Maximum Number of Versions for a Column Family
====
alter ‘t1′, NAME => ‘f1′, VERSIONS => 5
====
 
[TIP]
.Modify the Maximum Number of Versions for a Column Family
====
alter ‘t1′, NAME => ‘f1′, MIN_VERSIONS => 2
====	
	
==  key-hashing strategy

Readings : 
 * https://sematext.com/blog/2012/04/09/hbasewd-avoid-regionserver-hotspotting-despite-writing-records-with-sequential-keys/
 * https://www.slideshare.net/amansk/hbase-schema-design-big-data-techcon-boston
	
==== Problems

The critical issue of distributing your row keys well to avoid “hot” regions is well known.
As an example of not really well distributed tables, let’s assume you need to store per-user data. 
In this case, the row key would simply be the user’s ID, which will probably be a monotonically increasing integer (i.e. generated using a sequence, using MySQL or other tools). It’s easy to see that having the user ID as the key would make all writes for new users go into the last region of your table, which handles the highest values. Additionally, if new users tend to be significantly more active than older ones, or vice-versa, then any updates to existing rows won’t be well distributed across regions as well. Substitute the term ‘User ID’ with ‘Ticket ID’, ‘Product ID’ or any other entity type where there’s a much higher than average write-rate for a small portion of IDs, and you might discover this issue in your own use-cases. 
	
==== Simple solution
However, if we examine that monotonic ID’s structure more closely, we’ll see that it does contain an element that cycles nicely and evenly with each new ID allocated: its least-significant byte. However, once you have any significant number of users, the most significant bytes of generated IDs remain pretty constant for long periods of time while only the least significant bytes rotate (I’m assuming big-endian order, which is pretty much the standard for binary serialization; see for example Hadoop’s Bytes class and Java’s DataOutputStream). This is unfortunate, because good distribution of keys relies on their most significant byte(s). To better illustrate this, think of the odometer in your car, whether digital or old school: how often would you see any of the left-most digits rotate?

speedometer
A fix for making these row keys distribute nicely is fairly easy to implement. You simply need to prefix the key with a leading byte based on the user ID, whose value is well distributed. In other words, you need a consistent hash. For any given ID, you should always get back the same value.

One way of achieving this is to define a fixed number of buckets, with the leading byte in the key being the bucket number. That byte is usually calculated as userId % BUCKETS_NUMBER. This in effect relies on the well distributed nature of the lowest byte, so alternatively you could just grab the whole least significant byte of the ID as the prefix.

If you have a table for collecting a user’s raw events for later per-user aggregation, then having a consistent hash also has another advantage. It guarantees that a user’s data will always reside under the same prefix, so you can write concurrent code that processes each prefix (i.e. each block of users) without the need for a later reduce phase between tasks. Of course, a MapReduce job could easily merge a user’s rows using the user ID as key, however at Dynamic Yield we’ve tried to steer clear from M/R when dealing with jobs that need to run frequently and quickly, given the high overhead of launching MapReduce. (We’re currently switching to Apache Spark for this kind of job). Whether you use M/R or custom parallel code, you probably want to ensure each task gets an equal share of work, which is another advantage of well distributed tables.

However, sometimes there’s no need for a consistent hash. Assume you have a table whose native key is simply the timestamp (for later scanning by time range). To avoid one hot region that handles all new writes, you could simply generate a random byte (with a value smaller than BUCKETS_NUMBER)as the prefix when writing a new row. To then perform a partial scan for any given time range, you would need a separate scan for each prefix. Note that this multi-scan approach significantly differs from using the built-in scan.setTimeRange() method to find all data with a given HBase-timestamp range (regardless of the row key). The latter requires the Region Server to perform intense analysis over much of the table’s data in order to filter out any data not in range. For large tables, this might mean a very slow scan.
	

=== Monotonically Increasing Row Keys/Timeseries Data

In the HBase chapter of Tom White’s book Hadoop: The Definitive Guide (O’Reilly) there is a an optimization note on watching out for a phenomenon where an import process walks in lock-step with all clients in concert pounding one of the table’s regions (and thus, a single node), then moving onto the next region, etc. With monotonically increasing row-keys (i.e., using a timestamp), this will happen. See this comic by IKai Lan on why monotonically increasing row keys are problematic in BigTable-like datastores: monotonically increasing values are bad. The pile-up on a single region brought on by monotonically increasing keys can be mitigated by randomizing the input records to not be in sorted order, but in general it’s best to avoid using a timestamp or a sequence (e.g. 1, 2, 3) as the row-key.

If you do need to upload time series data into HBase, you should study OpenTSDB as a successful example. It has a page describing the schema it uses in HBase. The key format in OpenTSDB is effectively [metric_type][event_timestamp], which would appear at first glance to contradict the previous advice about not using a timestamp as the key. However, the difference is that the timestamp is not in the lead position of the key, and the design assumption is that there are dozens or hundreds (or more) of different metric types. Thus, even with a continual stream of input data with a mix of metric types, the Puts are distributed across various points of regions in the table.
	
=== Reverse Timestamps
	
Reverse Scan API

HBASE-4811 implements an API to scan a table or a range within a table in reverse, reducing the need to optimize your schema for forward or reverse scanning. This feature is available in HBase 0.98 and later. See https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html#setReversed%28boolean for more information.

A common problem in database processing is quickly finding the most recent version of a value. A technique using reverse timestamps as a part of the key can help greatly with a special case of this problem. Also found in the HBase chapter of Tom White’s book Hadoop: The Definitive Guide (O’Reilly), the technique involves appending (Long.MAX_VALUE - timestamp) to the end of any key, e.g. [key][reverse_timestamp].

The most recent value for [key] in a table can be found by performing a Scan for [key] and obtaining the first record. Since HBase keys are in sorted order, this key sorts before any older row-keys for [key] and thus is first.

This technique would be used instead of using Number of Versions where the intent is to hold onto all versions "forever" (or a very long time) and at the same time quickly obtain access to any other version by using the same Scan technique.


=== Pre- split tables

If you pre-split your table, it is critical to understand how your rowkey will be distributed across the region boundaries. As an example of why this is important, consider the example of using displayable hex characters as the lead position of the key (e.g., "0000000000000000" to "ffffffffffffffff"). Running those key ranges through Bytes.split (which is the split strategy used when creating regions in Admin.createTable(byte[] startKey, byte[] endKey, numRegions) for 10 regions will generate the following splits…​

 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48                                // 0
 54 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10                 // 6
 61 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -68                 // =
 68 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -126  // D
 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 72                                // K
 82 18 18 18 18 18 18 18 18 18 18 18 18 18 18 14                                // R
 88 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -44                 // X
 95 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -102                // _
 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102                // f

(note: the lead byte is listed to the right as a comment.) Given that the first split is a '0' and the last split is an 'f', everything is great, right? Not so fast.

The problem is that all the data is going to pile up in the first 2 regions and the last region thus creating a "lumpy" (and possibly "hot") region problem. To understand why, refer to an ASCII Table. '0' is byte 48, and 'f' is byte 102, but there is a huge gap in byte values (bytes 58 to 96) that will never appear in this keyspace because the only values are [0-9] and [a-f]. Thus, the middle regions will never be used. To make pre-splitting work with this example keyspace, a custom definition of splits (i.e., and not relying on the built-in split method) is required.

[TIP]
 Lesson #1: Pre-splitting tables is generally a best practice, but you need to pre-split them in such a way that all the regions are accessible in the keyspace. While this example demonstrated the problem with a hex-key keyspace, the same problem can happen with any keyspace. Know your data.

[TIP]
 Lesson #2: While generally not advisable, using hex-keys (and more generally, displayable data) can still work with pre-split tables as long as all the created regions are accessible in the keyspace.

To conclude this example, the following is an example of how appropriate splits can be pre-created for hex-keys:.

[Code,Java]
----
public static boolean createTable(Admin admin, HTableDescriptor table, byte[][] splits)
throws IOException {
  try {
    admin.createTable( table, splits );
    return true;
  } catch (TableExistsException e) {
    logger.info("table " + table.getNameAsString() + " already exists");
    // the table already exists...
    return false;
  }
}

public static byte[][] getHexSplits(String startKey, String endKey, int numRegions) {
  byte[][] splits = new byte[numRegions-1][];
  BigInteger lowestKey = new BigInteger(startKey, 16);
  BigInteger highestKey = new BigInteger(endKey, 16);
  BigInteger range = highestKey.subtract(lowestKey);
  BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));
  lowestKey = lowestKey.add(regionIncrement);
  for(int i=0; i < numRegions-1;i++) {
    BigInteger key = lowestKey.add(regionIncrement.multiply(BigInteger.valueOf(i)));
    byte[] b = String.format("%016x", key).getBytes();
    splits[i] = b;
  }
  return splits;
}
----

	
== Secondary indexes

Some biblio can be found here : 
	TODO: HBase FuzzyRowFilter: Alternative to Secondary Indexes	
	ref : https://sematext.com/blog/2012/08/09/consider-using-fuzzyrowfilter-when-in-need-for-secondary-indexes-in-hbase/

=== Filter Query

Depending on the case, it may be appropriate to use Client Request Filters. In this case, no secondary index is created. However, don’t try a full-scan on a large table like this from an application (i.e., single-threaded client).

=== Periodic-Update Secondary Index

A secondary index could be created in another table which is periodically updated via a MapReduce job. The job could be executed intra-day, but depending on load-strategy it could still potentially be out of sync with the main data table.

See mapreduce.example.readwrite for more information.

=== Dual-Write Secondary Index

Another strategy is to build the secondary index while publishing data to the cluster (e.g., write to data table, write to index table). If this is approach is taken after a data table already exists, then bootstrapping will be needed for the secondary index with a MapReduce job (see secondary.indexes.periodic).

=== Summary Tables

Where time-ranges are very wide (e.g., year-long report) and where the data is voluminous, summary tables are a common approach. These would be generated with MapReduce jobs into another table.

See mapreduce.example.summary for more information.

=== Coprocessor Secondary Index

Coprocessors act like RDBMS triggers. These were added in 0.92. For more information, see coprocessors


== Case studies 

Some initiatic examples can be found at :

http://hbase.apache.org/1.2/book.html#schema.casestudies

(If this is a bit confusing, take an hour and watch Lars George’s excellent video about understanding HBase schema design: http://www.youtube.com/watch?v=_HLoH_PgrLk).

https://www.eduonix.com/blog/bigdata-and-hadoop/learn-develop-effective-data-models-hbase/




=== DBA Tips

.Activate compression :

  ALTER TABLE 'test', {NAME=>'mycolumnfamily', COMPRESSION=>'SNAPPY'} 

.Data block encoding of keys/values

 ALTER TABLE 'test', {NAME=>'mycolumnfamily', DATA_BLOCK_ENCODING => 'FAST_DIFF'}

.Change Split policy for a table (for Hbase 0.94+ the default Split policy changed from ConstantSizeRegionSplitPolicy (based on hbase.hregion.max.filesize) to IncreasingToUpperBoundRegionSplitPolicy)

 alter 'access_demo', {METHOD => 'table_att', CONFIGURATION => {'SPLIT_POLICY' => 'org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy'}}

Remember split will occur if the data size of a ColumnFamily gets bigger than the number defined by the policy.
  
== Some rules


There are many different data sets, with different access patterns and service-level expectations. Therefore, these rules of thumb are only an overview. Read the rest of this chapter to get more details after you have gone through this list.

 * Aim to have regions sized between 10 and 50 GB.
 * Aim to have cells no larger than 10 MB, or 50 MB if you use mob. Otherwise, consider storing your cell data in HDFS and store a pointer to the data in HBase.
 * A typical schema has between 1 and 3 column families per table. HBase tables should not be designed to mimic RDBMS tables.
 * Around 50-100 regions is a good number for a table with 1 or 2 column families. Remember that a region is a contiguous segment of a column family.
 * Keep your column family names as short as possible. The column family names are stored for every value (ignoring prefix encoding). They should not be self-documenting and descriptive like in a typical RDBMS.
 * If you are storing time-based machine data or logging information, and the row key is based on device ID or service ID plus time, you can end up with a pattern where older data regions never have additional writes beyond a certain age. In this type of situation, you end up with a small number of active regions and a large number of older regions which have no new writes. For these situations, you can tolerate a larger number of regions because your resource consumption is driven by the active regions only.
 * If only one column family is busy with writes, only that column family accomulates memory. Be aware of write patterns when allocating resources.

  
  
== Fancy stuff

=== Time To Live (TTL)

ColumnFamilies can set a TTL length in seconds, and HBase will automatically delete rows once the expiration time is reached. This applies to all versions of a row - even the current one. The TTL time encoded in the HBase for the row is specified in UTC.

Store files which contains only expired rows are deleted on minor compaction. Setting hbase.store.delete.expired.storefile to false disables this feature. Setting minimum number of versions to other than 0 also disables this.

See HColumnDescriptor for more information.

Recent versions of HBase also support setting time to live on a per cell basis. See HBASE-10560 for more information. Cell TTLs are submitted as an attribute on mutation requests (Appends, Increments, Puts, etc.) using Mutation#setTTL. If the TTL attribute is set, it will be applied to all cells updated on the server by the operation. There are two notable differences between cell TTL handling and ColumnFamily TTLs:

[TIP]
 Cell TTLs are expressed in units of milliseconds instead of seconds.

[TIP]
   A cell TTLs cannot extend the effective lifetime of a cell beyond a ColumnFamily level TTL setting.

  
  
= Comming from the relational world

Reference : https://mapr.com/blog/guidelines-hbase-schema-design/

There is no one-to-one mapping from relational databases to HBase. In relational design, the focus and effort is around describing the entity and its interaction with other entities; the queries and indexes are designed later.

With HBase, you have a “query-first” schema design; all possible queries should be identified first, and the schema model designed accordingly. You should design your HBase schema to take advantage of the strengths of HBase. Think about your access patterns, and design your schema so that the data that is read together is stored together. Remember that HBase is designed for clustering.


 * Distributed data is stored and accessed together
 * It is query-centric, so focus on how the data is read
 * Design for the questions

== Normalization

In a relational database, you normalize the schema to eliminate redundancy by putting repeating information into a table of its own. This has the following benefits:

 * You don’t have to update multiple copies when an update happens, which makes writes faster.
 * You reduce the storage size by having a single copy instead of multiple copies.

However, this causes joins. Since data has to be retrieved from more tables, queries can take more time to complete.

In this example below, we have an order table which has one-to-many relationship with an order items table. The order items table has a foreign key with the id of the corresponding order.

image::Hbase-Normalisation.png[Hbase Normalisation]

== De Normalisation

In a de-normalized datastore, you store in one table what would be multiple indexes in a relational world. De-normalization can be thought of as a replacement for joins. Often with HBase, you de-normalize or duplicate data so that data is accessed and stored together.

=== Parent-Child Relationship–Nested Entity

Here is an example of denormalization in HBase, if your tables exist in a one-to-many relationship, it’s possible to model it in HBase as a single row. In the example below, the order and related line items are stored together and can be read together with a get on the row key. This makes the reads a lot faster than joining tables together.

image:Hbase_nested_entity.png[Hbase nested entity]

The rowkey corresponds to the parent entity id, the OrderId. There is one column family for the order data, and one column family for the order items. The Order Items are nested, the Order Item IDs are put into the column names and any non-identifying attributes are put into the value.

This kind of schema design is appropriate when the only way you get at the child entities is via the parent entity.

=== Many-to-Many Relationship in an RDBMS

Here is an example of a many-to-many relationship in a relational database. These are the query requirements:

 * Get name for user x
 * Get title for book x
 * Get books and corresponding ratings for userID x
 * Get all userIDs and corresponding ratings for book y


image::Hbase-bookStore.png[Hbase - book store example]

=== Many-to-Many Relationship in HBase

The queries that we are interested in are:

 * Get books and corresponding ratings for userID x
 * Get all userIDs and corresponding ratings for book y

For an entity table, it is pretty common to have one column family storing all the entity attributes, and column families to store the links to other entities.

The entity tables are as shown below:

image::Hbase-bookStoreHbase.png[The book store in hbase]


== Generic Data, Event Data, and Entity-Attribute-Value

Generic data that is schemaless is often expressed as name value or entity attribute value. In a relational database, this is complicated to represent. A conventional relational table consists of attribute columns that are relevant for every row in the table, because every row represents an instance of a similar object. A different set of attributes represents a different type of object, and thus belongs in a different table. The advantage of HBase is that you can define columns on the fly, put attribute names in column qualifiers, and group data by column families.

Here is an example of clinical patient event data. The Row Key is the patient ID plus a time stamp. The variable event type is put in the column qualifier, and the event measurement is put in the column value. OpenTSDB is an example of variable system monitoring data.

image::Hbase-GenericDataEvent.png[Generic data event]

== Self-Join Relationship – HBase

A self-join is a relationship in which both match fields are defined in the same table.

Consider a schema for twitter relationships, where the queries are: which users does userX follow, and which users follow userX? Here’s a possible solution: The userids are put in a composite row key with the relationship type as a separator. For example, Carol follows Steve Jobs and Carol is followed by BillyBob. This allows for row key scans for everyone carol:follows or carol:followedby

Below is the example Twitter table:
image::Hbase-twitterExample.png[Twitter Example]


== Tree, Graph Data

Here is an example of an adjacency list or graph, using a separate column for each parent and child:

image::Hbase-Graph.png[Hbase graph]

Each row shows a node, and the row key is equal to the node id. There is a column family for parent p, and a column family children c. The column qualifiers are equal to the parent or child node ids, and the value is equal to the type to node. This allows to quickly find the parent or children nodes from the row key.

You can see there are multiple ways to represent trees, the best way depends on your queries.

== Inheritance Mapping

In this online store example, the type of product is a prefix in the row key. Some of the columns are different, and may be empty depending on the type of product. This allows to model different product types in the same table and to scan easily by product type.

image::Hbase-InheritanceMapping.png[InheritanceMapping]




==== Linear write time

 put 'musicband', 'U2-Paris-1980-7', 'show:Number', '7'
 0 row(s) in 0.0042 seconds
 ....
 ...
 put 'musicband', 'U2-Paris-1980-999997', 'show:Number', '999997'
 0 row(s) in 0.0040 seconds



= Programming with hbase



== Shell

=== shell from text file

You can enter HBase Shell commands into a text file, one command per line, and pass that file to the HBase Shell.

.Example command file

====
 create 'test', 'cf'
 list 'test'
 put 'test', 'row1', 'cf:a', 'value1'
 put 'test', 'row2', 'cf:b', 'value2'
 put 'test', 'row3', 'cf:c', 'value3'
 put 'test', 'row4', 'cf:d', 'value4'
 scan 'test'
 get 'test', 'row1'
 disable 'test'
 enable 'test'
====

.run command from file
====
 ./hbase shell ./sample_commands.txt
====


===  A detailed documentation of the commands is available here

https://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/


=== Beining efficient with the shell 

==== configuration file 

irbrc file-irbrc configuration to save all command history of all hbase shell invocations.


.minimal configuration of irbrc-

[source]
----
more ~/.irbrc
require 'irb/ext/save-history'
IRB.conf[:SAVE_HISTORY] = 100
IRB.conf[:HISTORY_FILE] = "#{ENV['HOME']}/.irb_history"
Kernel.at_exit do
    IRB.conf[:AT_EXIT].each do |i|
        i.call
    end
end
----

==== enabling debug model

[source]
-----
hbase>debug
or
./bin/hbase shell -d
-----

==== counters

counters with hbase- hbase offers counter feature, counters are very useful in statistics


[source]
-----
hbase(main):001:0> create 'account', 'id'
0 row(s) in 1.1930 seconds
hbase(main):002:0> incr 'account', '2014', 'id:n', 1
COUNTER VALUE = 1
hbase(main):04:0> get_counter 'account', '2014', 'id:n'
COUNTER VALUE = 2
-----

==== avoid full scan row  : scan query optimization


Scan is used to get the data from hbase and the costliest operation.
An optional startRow and stopRow is useful to improve the query performance.If rows are not defined(start and stop), the Scanner will iterate over all rows.
Hbase scan queries with start and end key are much faster because, it doesn’t have to scan everything to get the specified query/filter data.
Here is tricks-

[source]
-----
    create hbase table and populate data-

    create 'TS','cf'
-----

the result will be 
.Table populated
|===
|card_number_year_month_day_time_o |transaction_amt|location|type|year|month

|100_2014_06_10_10_932845_ta
|100
|bangalore
|credit
|2014
|6

|23989_2000_01_11_10_5468756_ta
|45843745
|bangalore india
|debit
|2000
|5

|487545_2000_01_11_10_5468756_ta
|
|
|
|2000
|1
|===


Avoid Full Table Scan-

find out all transaction done by card number x at place bangalore.
use prefix/rowkey filter with regex/substring comparator to set the search condition and set the start row as ‘X’ and stop row ‘X~’.
Row keys are sorted(lexical) and data is stored in byte in hbase. The start/stop key helps to avoid the complete table scan and fetch the data from region contains the range value, as(~) is last in ascii table so hbase scan lookup the rows having prefix X~.
Retrieving data from HBase scan with filter-

[source]
-----
    Scan scan = new Scan(Bytes.ToBytes("23989"),Bytes.toBytes("23989~");
    scan.setFilter(...);
-----

Disable cache at client-

	
[source]
-----
    setCacheBlocks(false)
    and setCaching(0) 
-----

Get all the row having account number 23989


[source]
-----
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.RowFilter
import org.apache.hadoop.hbase.filter.SubstringComparator
scan 'TS', {STARTROW=>'23989', STOPROW=>'23989~',FILTER=>RowFilter.new(CompareFilter::CompareOp.valueOf('EQUAL'),SubstringComparator.new('23989'))}
-----

Use start and stop row to optimize scan query.


== Java


There's several good ressources to start with available 

 * http://www.informit.com/articles/article.aspx?p=2255108&seqNum=2
 * https://autofei.wordpress.com/2012/04/02/java-example-code-using-hbase-data-model-operations/
 * https://www.tutorialspoint.com/hbase/hbase_read_data.htm

=== Let's start with maven

.pom file

[source,xml]
-----
<project xmlns="http://maven.apache.org/POM/4.0.0" 
	     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.zenika</groupId>
    <artifactId>hbase-example</artifactId>
    <version>1.0-SNAPSHOT</version>
    <packaging>jar</packaging>

    <name>hbase-example</name>
    <url>http://maven.apache.org</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
             <groupId>org.apache.hbase</groupId>
             <artifactId>hbase-client</artifactId>
             <version>0.98.5-hadoop2</version>
        </dependency>

        <dependency>
             <groupId>junit</groupId>
             <artifactId>junit</artifactId>
             <version>4.11</version>
             <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.0.2</version>
                <configuration>
                    <source>1.6</source>
                    <target>1.6</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <configuration>
                   <archive>
                       <manifest>
                           <addClasspath>true</addClasspath>
                           <classpathPrefix>lib/</classpathPrefix>
                           <mainClass>com.zenika.hbaseexample.HBaseExample</mainClass>
                       </manifest>
                    </archive>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-dependency-plugin</artifactId>
                <executions>
                    <execution>
                        <id>copy</id>
                        <phase>install</phase>
                        <goals>
                            <goal>copy-dependencies</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}/lib</outputDirectory>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
-----

.compile install and run 

[source,shell]
-----
   mvn clean install
-----

This creates a file in the target directory named hbase-example-1.0-SNAPSHOT.jar. You can execute it with the following command:

[source,shell]
-----
  java -jar hbase-example-1.0-SNAPSHOT.jar
-----

=== The POJO page view

[source,java]
-----
package com.zenika.hbaseexample;

public class PageView
{

    private String userId;
    private String page;

    public PageView() {
    }

    public PageView(String userId, String page) {
        this.userId = userId;
        this.page = page;

    }
    public String getUserId() {
        return userId;
    }

    public void setUserId(String userId) {
        this.userId = userId;
    }
    public String getPage() {
        return page;
    }

    public void setPage(String page) {
        this.page = page;
    }

}
-----

=== The main class

[source,java]
-----
package com.zenika.hbaseexample;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class HBaseExample
{

    private HTableInterface pageViewTable;

    public HBaseExample()
    {
        try
        {
            Configuration conf = HBaseConfiguration.create();
			//you may need this specific configuration
			//conf.set("hbase.zookeeper.quorum", "server’s IP address");
            pageViewTable = new HTable( conf, "PageViews");
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
    }

    public void close()
    {
        try

        {
            pageViewTable.close();
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
    }

    public void put( PageView pageView )
    {
        // Create a new Put object with the Row Key as the bytes of the user id
        Put put = new Put( Bytes.toBytes( pageView.getUserId() ) );

        // Add the user id to the info column family
        put.add( Bytes.toBytes( "info" ),
                 Bytes.toBytes( "userId" ),
                 Bytes.toBytes( pageView.getUserId() ) );

        // Add the page to the info column family
        put.add( Bytes.toBytes( "info" ),
                 Bytes.toBytes( "page" ),
                 Bytes.toBytes( pageView.getPage() ) );
        try

        {

            // Add the PageView to the page view table
            pageViewTable.put( put );
        }
        catch( IOException e )
        {
            e.printStackTrace();
        }
    }

    public PageView get( String rowkey )

    {
        try
        {

            // Create a Get object with the rowkey (as a byte[])
            Get get = new Get( Bytes.toBytes( rowkey ) );

            // Execute the Get
            Result result = pageViewTable.get( get );

            // Retrieve the results
            PageView pageView = new PageView();
            byte[] bytes = result.getValue( Bytes.toBytes( "info" ),
                                            Bytes.toBytes( "userId" ) );
            pageView.setUserId( Bytes.toString( bytes ) );
            bytes = result.getValue( Bytes.toBytes( "info" ),
                                     Bytes.toBytes( "page" ) );
            pageView.setPage(Bytes.toString(bytes));


            // Return the newly constructed PageView
            return pageView;
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
        return null;
    }
    public void delete( String rowkey )
    {
        try
        {
            Delete delete = new Delete( Bytes.toBytes( rowkey ) );
            pageViewTable.delete( delete );
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
    }

    public List<PageView> scan( String startRowKey, String endRowKey )
    {
        try
        {
            // Build a list to hold our results
            List<PageView> pageViewResults = new ArrayList<PageView>();


            // Create and execute a scan
            Scan scan = new Scan( Bytes.toBytes( startRowKey ), Bytes.toBytes( endRowKey ) );
            ResultScanner results = pageViewTable.getScanner(scan);
IsetPage(Bytes.toString(bytes));

                // Add the PageView to our results
                pageViewResults.add( pageView );
            }

            // Return our results
            return pageViewResults;
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
        return null;
    }

    public static void main( String[] args )

    {
        HBaseExample example = new HBaseExample();

        // Create two records
        example.put( new PageView( "User1", "/mypage" ) );
        example.put( new PageView( "User2","/mypage" ) );

        // Execute a Scan from "U" to "V"
        List<PageView> pageViews = example.scan( "U", "V" );
        if( pageViews != null ) {
            System.out.println("Page Views:");
            for (PageView pageView : pageViews) {
                System.out.println("\tUser ID: " + pageView.getUserId() + ", Page: " + pageView.getPage());
            }
        }

        // Get a specific row
        PageView pv = example.get( "User1" );
        System.out.println( "User ID: " + pv.getUserId() + ", Page: " + pv.getPage() );

        // Delete a row
        example.delete( "User1" );

        // Execute another scan, which should just have User2 in it
        pageViews = example.scan( "U", "V" );
        if( pageViews != null ) {
            System.out.println("Page Views:");
            for (PageView pageView : pageViews) {
                System.out.println("\tUser ID: " + pageView.getUserId() + ", Page: " + pageView.getPage());
            }
        }

        // Close our table
        example.close();
    }
}
-----

=== unit testing 

https://blog.cloudera.com/blog/2013/09/how-to-test-hbase-applications-using-popular-tools/
https://github.com/apache/hbase/blob/master/src/main/asciidoc/_chapters/unit_testing.adoc
https://github.com/dbist/HBaseUnitTest  <= 2016
http://hbase.apache.org/0.94/book/hbase.tests.html

== PHP




