= HBase  
:toc:

== What's HBASE

A non-relational (NoSQL) database that runs on top of HDFS

Apache HBase is an open source NoSQL database that provides real-time read/write access to those large datasets.

HBase scales linearly to handle huge data sets with billions of rows and millions of columns, and it easily combines data sources that use a wide variety of different structures and schemas. HBase is natively integrated with Hadoop and works seamlessly alongside other data access engines through YARN.
What HBase Does

Apache HBase provides random, real time access to your data in Hadoop. It was created for hosting very large tables, making it a great choice to store multi-structured or sparse data. Users can query HBase for a particular point in time, making “flashback” queries possible. These following characterisitcs make HBase a great choice for storing semi-structured data like log data and then providing that data very quickly to users or applications integrated with HBase.

 * Fault tolerant 	

	Replication across the data center
    Atomic and strongly consistent row-level operations
    High availability through automatic failover
    Automatic sharding and load balancings of tables
	
 * Scalling 

    Supports scaling out in coordination with Hadoop file system even on commodity hardware
	Very flexible on schema design/no fixed schema

 * Fast 	

    Near real time lookups
    In-memory caching via block cache and bloom filters
    Server side processing via filters and co-processors
	
 * Usable 	
 
    Data model accommodates wide range of use cases
    Metrics exports via File and Ganglia plugins
    Easy Java API as well as Thrift and REST gateway APIs
	Great for analytics in association with Hadoop MapReduce
	Can be integrated with Hive for SQL-like queries, which is better for DBAs who are more familiar with SQL queries
	
  * CONS	
	
	Single point of failure (when only one HMaster is used)
    No transaction support
    JOINs are handled in MapReduce layer rather than the database itself
    Indexed and sorted only on key, but RDBMS can be indexed on some arbitrary field
    No built-in authentication or permissions

== Versions 

 * 2.0.0 (soon)
 * 1.3.0 (latest)
 * 1.2.4 (stable)

== Introduction 

HBase quick start, good practices 

From official references 

 * http://hbase.apache.org/book.html
 * Download link : http://www.apache.org/dyn/closer.cgi/hbase/

From tutorials

 *  http://www.guru99.com/hbase-tutorials.html
 * http://hortonworks.com/apache/hbase/
 * https://hbase.apache.org/book.html
 
From Pro and cons
 
 * https://www.packtpub.com/mapt/book/big-data-and-business-intelligence/9781783985944/1/ch01lvl1sec16/hbase-pros-and-cons
 * http://www.cyanny.com/2014/03/13/hbase-architecture-analysis-part-3-pros-cons/
 * http://stackoverflow.com/questions/22542307/hbase-what-are-the-pros-and-cons-of-using-one-column-with-a-list-of-values-vs
 * http://www.slideshare.net/EdurekaIN/no-sql-databases-35591065

Data Model 

 * http://jimbojw.com/#understanding%20hbase
 * http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/9353-login1210_khurana.pdf
 
Performance

 * https://db-blog.web.cern.ch/blog/zbigniew-baranowski/2017-01-performance-comparison-different-file-formats-and-storage-engines
 
Tips 

 * https://www.dynamicyield.com/2015/05/apache-hbase-for-the-win-2/
 * http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/
 * http://www.techsquids.com/bd/hbase-scan-filters-tips-tricks/
 * http://lecluster.delaurent.com/hbase-tips-tricks/
 * https://intellipaat.com/interview-question/hbase-interview-questions/
 * https://dzone.com/articles/handling-big-data-hbase-part-5
 * http://www.slideshare.net/lhofhansl/h-base-tuninghbasecon2015ok

== Key concepts

=== Master, zookeeper, region servers

TODO: define zookeeper and region servers



 
=== Understanding HBASE and BigTable (from Jim R. Wilson)

May 2008

The hardest part about learning HBase (the open source implementation of Google's BigTable), is just wrapping your mind around the concept of what it actually is.

I find it rather unfortunate that these two great systems contain the words table and base in their names, which tend to cause confusion among RDBMS indoctrinated individuals (like myself).

This article aims to describe these distributed data storage systems from a conceptual standpoint. After reading it, you should be better able to make an educated decision regarding when you might want to use HBase vs when you'd be better off with a "traditional" database.
it's all in the terminology

Fortunately, Google's BigTable Paper clearly explains what BigTable actually is. Here is the first sentence of the "Data Model" section:

    A Bigtable is a sparse, distributed, persistent multidimensional sorted map.

Note: At this juncture I like to give readers the opportunity to collect any brain matter which may have left their skulls upon reading that last line.

The BigTable paper continues, explaining that:

    The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes.

Along those lines, the HBaseArchitecture page of the Hadoop wiki posits that:

    HBase uses a data model very similar to that of Bigtable. Users store data rows in labelled tables. A data row has a sortable key and an arbitrary number of columns. The table is stored sparsely, so that rows in the same table can have crazily-varying columns, if the user likes.

Although all of that may seem rather cryptic, it makes sense once you break it down a word at a time. I like to discuss them in this sequence: map, persistent, distributed, sorted, multidimensional, and sparse.

Rather than trying to picture a complete system all at once, I find it easier to build up a mental framework piecemeal, to ease into it...

==== map

At its core, HBase/BigTable is a map. Depending on your programming language background, you may be more familiar with the terms associative array (PHP), dictionary (Python), Hash (Ruby), or Object (JavaScript).

From the wikipedia article, a map is "an abstract data type composed of a collection of keys and a collection of values, where each key is associated with one value."

Using JavaScript Object Notation, here's an example of a simple map where all the values are just strings:

[source,json,subs="verbatim,attributes"]
----
{
  "zzzzz" : "woot",
  "xyz" : "hello",
  "aaaab" : "world",
  "1" : "x",
  "aaaaa" : "y"
}
----

==== persistent

Persistence merely means that the data you put in this special map "persists" after the program that created or accessed it is finished. This is no different in concept than any other kind of persistent storage such as a file on a filesystem. Moving along...

==== distributed

HBase and BigTable are built upon distributed filesystems so that the underlying file storage can be spread out among an array of independent machines.

HBase sits atop either Hadoop's Distributed File System (HDFS) or Amazon's Simple Storage Service (S3), while a BigTable makes use of the Google File System (GFS).

Data is replicated across a number of participating nodes in an analogous manner to how data is striped across discs in a RAID system.

For the purpose of this article, we don't really care which distributed filesystem implementation is being used. The important thing to understand is that it is distributed, which provides a layer of protection against, say, a node within the cluster failing.

==== sorted

Unlike most map implementations, in HBase/BigTable the key/value pairs are kept in strict alphabetical order. That is to say that the row for the key "aaaaa" should be right next to the row with key "aaaab" and very far from the row with key "zzzzz".
Continuing our JSON example, the sorted version looks like this:

[source,json,subs="verbatim,attributes"]
----
	
{
  "1" : "x",
  "aaaaa" : "y",
  "aaaab" : "world",
  "xyz" : "hello",
  "zzzzz" : "woot"
}
----

Because these systems tend to be so huge and distributed, this sorting feature is actually very important. The spacial propinquity of rows with like keys ensures that when you must scan the table, the items of greatest interest to you are near each other.

This is important when choosing a row key convention. For example, consider a table whose keys are domain names. It makes the most sense to list them in reverse notation (so "com.jimbojw.www" rather than "www.jimbojw.com") so that rows about a subdomain will be near the parent domain row.

Continuing the domain example, the row for the domain "mail.jimbojw.com" would be right next to the row for "www.jimbojw.com" rather than say "mail.xyz.com" which would happen if the keys were regular domain notation.

It's important to note that the term "sorted" when applied to HBase/BigTable does not mean that "values" are sorted. There is no automatic indexing of anything other than the keys, just as it would be in a plain-old map implementation.

==== multidimensional

Up to this point, we haven't mentioned any concept of "columns", treating the "table" instead as a regular-old hash/map in concept. This is entirely intentional. The word "column" is another loaded word like "table" and "base" which carries the emotional baggage of years of RDBMS experience.

Instead, I find it easier to think about this like a multidimensional map - a map of maps if you will. Adding one dimension to our running JSON example gives us this:

[source,json,subs="verbatim,attributes"]
----
{
  "1" : {
    "A" : "x",
    "B" : "z"
  },
  "aaaaa" : {
    "A" : "y",
    "B" : "w"
  },
  "aaaab" : {
    "A" : "world",
    "B" : "ocean"
  },
  "xyz" : {
    "A" : "hello",
    "B" : "there"
  },
  "zzzzz" : {
    "A" : "woot",
    "B" : "1337"
  }
}
----

In the above example, you'll notice now that each key points to a map with exactly two keys: "A" and "B". From here forward, we'll refer to the top-level key/map pair as a "row". Also, in BigTable/HBase nomenclature, the "A" and "B" mappings would be called "Column Families".

A table's column families are specified when the table is created, and are difficult or impossible to modify later. It can also be expensive to add new column families, so it's a good idea to specify all the ones you'll need up front.

Fortunately, a column family may have any number of columns, denoted by a column "qualifier" or "label". Here's a subset of our JSON example again, this time with the column qualifier dimension built in:

[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "aaaaa" : {
    "A" : {
      "foo" : "y",
      "bar" : "d"
    },
    "B" : {
      "" : "w"
    }
  },
  "aaaab" : {
    "A" : {
      "foo" : "world",
      "bar" : "domination"
    },
    "B" : {
      "" : "ocean"
    }
  },
  // ...
}
----

Notice that in the two rows shown, the "A" column family has two columns: "foo" and "bar", and the "B" column family has just one column whose qualifier is the empty string ("").

When asking HBase/BigTable for data, you must provide the full column name in the form "family:qualifier". So for example, both rows in the above example have three columns: "A:foo", "A:bar" and "B:".

Note that although the column families are static, the columns themselves are not. Consider this expanded row:

[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "zzzzz" : {
    "A" : {
      "catch_phrase" : "woot",
    }
  }
}
----

In this case, the "zzzzz" row has exactly one column, "A:catch_phrase". Because each row may have any number of different columns, there's no built-in way to query for a list of all columns in all rows. To get that information, you'd have to do a full table scan. You can however query for a list of all column families since these are immutable (more-or-less).

The final dimension represented in HBase/BigTable is time. All data is versioned either using an integer timestamp (seconds since the epoch), or another integer of your choice. The client may specify the timestamp when inserting data.

Consider this updated example utilizing arbitrary integral timestamps:


[source,json,subs="verbatim,attributes"]
----
{
  // ...
  "aaaaa" : {
    "A" : {
      "foo" : {
        15 : "y",
        4 : "m"
      },
      "bar" : {
        15 : "d",
      }
    },
    "B" : {
      "" : {
        6 : "w"
        3 : "o"
        1 : "w"
      }
    }
  },
  // ...
}
----

Each column family may have its own rules regarding how many versions of a given cell to keep (a cell is identified by its rowkey/column pair) In most cases, applications will simply ask for a given cell's data, without specifying a timestamp. In that common case, HBase/BigTable will return the most recent version (the one with the highest timestamp) since it stores these in reverse chronological order.

If an application asks for a given row at a given timestamp, HBase will return cell data where the timestamp is less than or equal to the one provided.

Using our imaginary HBase table, querying for the row/column of "aaaaa"/"A:foo" will return "y" while querying for the row/column/timestamp of "aaaaa"/"A:foo"/10 will return "m". Querying for a row/column/timestamp of "aaaaa"/"A:foo"/2 will return a null result.

==== sparse

The last keyword is sparse. As already mentioned, a given row can have any number of columns in each column family, or none at all. The other type of sparseness is row-based gaps, which merely means that there may be gaps between keys.

This, of course, makes perfect sense if you've been thinking about HBase/BigTable in the map-based terms of this article rather than perceived similar concepts in RDBMS's.

==== And that's about it

Well, I hope that helps you understand conceptually what the HBase data model feels like.

As always, I look forward to your thoughts, comments and suggestions.

 
 
= Installation

== Quick start for Installation 

Download the latest stable version from apache website.

Create an installation dir (the data storage is easily configurable)

My choice : 

 * centos 7
 * installation in my user home dir
 * configuration  of the data dir inside the _conf/hbase-site.xml_ file (see example bellow)
 * ensure java is installed, and JAVA_HOME is configured. export JAVA_HOME=/usr 

....
 <configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///opt/hbase/hbase</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/opt/hbase/zookeeper</value>
  </property>
</configuration>
....  

=== Start hbase

to start hbase simply run the startup script _bin/start-hbase.sh_
From this point you can access the administrative page : _lynx localhost:16010_ 
if you want to access it from an other server, you need to configure the firewall

....
sudo firewall-cmd --get-active-zones   #to list the zone where you have a firewall applicable
#need to configure it for all zone. Take care if you are in a dmz, or a secure area
sudo firewall-cmd --zone=public --add-port=16010/tcp --permanent
sudo firewall-cmd --reload
....

=== Connect to your local instance

simply run _./bin/hbase shell_


=== Create Table, put data, get results

To create a table : 
....
hbase(main):003:0> create 'test', 'cf'
0 row(s) in 1.4610 seconds

=> Hbase::Table - test
....

you can double check in the browser

image::hbase_createTable.png[hbase_createTable]

Now, we can add data, a get them 

....
hbase(main):002:0> put 'test', 'row1', 'cf:a', 'value1'
hbase(main):003:0> put 'test', 'row2', 'cf:b', 'value2'
hbase(main):004:0> put 'test', 'row3', 'cf:c', 'value3'

#to get the full content of the table
hbase(main):006:0> scan 'test'
ROW                               COLUMN+CELL
 row1                             column=cf:a, timestamp=1487598057519, value=value1
 row2                             column=cf:b, timestamp=1487598062099, value=value2
 row3                             column=cf:c, timestamp=1487598066972, value=value3
3 row(s) in 0.0220 seconds

#To get only one row
hbase(main):029:0* get 'test', 'row1'
COLUMN                            CELL
 cf:a                             timestamp=1487598057519, value=value1
1 row(s) in 0.0280 seconds
....

[TIP]
====
 before dropping a table, or alter it, you need to disable it.
 _disable 'test'_ 
====

=== Stop hbase

simply run _./bin/stop-hbase.sh_

== Over Hadoop, 




if you want to move to hdfs :
....
<property>
  <name>hbase.rootdir</name>
  <value>hdfs://localhost:8020/hbase</value>
</property>
....

you'll see a new bunch of files, hbase will create them automatically.
Warning, if there's already an HBase directory, hbase will try to do a migration from the existing version.


image::HbaseOverHadoop.png[HBase over hadoop]


== Now, let's go to an hbase cluster (we'll not discuss about hadoop cluster here ... )

=== First, separate zookeeper from the HBase master 

Update the _conf/hbase-site.xml_ with :

....
<property>
  <name>hbase.cluster.distributed</name>
  <value>true</value>
</property>
....

=== How to check it's working 

==== do you have jps (Java Virtual Machine Process Status Tool)

it's not installed by default with openJDK, you need to install the devel modules

 [admin@localhost hbase]$ sudo yum list *java*devel*
 Modules complémentaires chargés : fastestmirror, langpacks
 Loading mirror speeds from cached hostfile
  * base: miroir.univ-paris13.fr
  * extras: mirrors.ircam.fr
  * updates: miroir.univ-paris13.fr
 Paquets disponibles
 java-1.6.0-openjdk-devel.x86_64                     1:1.6.0.41-1.13.13.1.el7_3                updates
 java-1.7.0-openjdk-devel.x86_64                     1:1.7.0.131-2.6.9.0.el7_3                 updates
 java-1.8.0-openjdk-devel.i686                       1:1.8.0.121-0.b13.el7_3                   updates
 java-1.8.0-openjdk-devel.x86_64                     1:1.8.0.121-0.b13.el7_3                   updates
 java-1.8.0-openjdk-devel-debug.i686                 1:1.8.0.121-0.b13.el7_3                   updates
 java-1.8.0-openjdk-devel-debug.x86_64               1:1.8.0.121-0.b13.el7_3                   updates
 libdb-java-devel.i686                               5.3.21-19.el7                             base   
 libdb-java-devel.x86_64                             5.3.21-19.el7                             base   
 libguestfs-java-devel.x86_64                        1:1.32.7-3.el7.centos.2                   updates
 libvirt-java-devel.noarch                           0.4.9-4.el7                               base   

==== run jps command

 [admin@localhost hbase]$ jps
 10066 SecondaryNameNode
 10619 HMaster
 9741 NameNode
 9885 DataNode
 10541 HQuorumPeer
 10718 HRegionServer
 12494 Jps



=== You need 3 hosts to do such setup

== Configuration tips

A few configuration recommendations include disabling auto-compaction (by default it happens every 24 hours from the time you start HBase) and schedule it to run every day at an off-peak time. You should also configure compression (such as LZO) and explicitly put the correctly configured HBase conf directory in your CLASSPATH.


== Metrics 

ref : http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/
You should also keep the number of regions to a reasonable number based on memstore size and amount of RAM and the RegionServer JVM should be limited to 12GB of java heap to minimize long GC pauses. For example a machine with 36GB of RAM that is also running a DataNode daemon could handle approximately 100 regions with active writes and a memstore of 48MB each. That allows enough headroom for DataNode and RegionServer memory requirements, Linux file buffer space and a reasonable flush size for each RegionServer.


= DATA MODEL

== Concepts

TODO: http://jimbojw.com/#understanding%20hbase

.Namespace 

	A namespace is a logical grouping of tables analogous to a database in relation database systems. This abstraction lays the groundwork for upcoming multi-tenancy related features:
 * Quota Management (HBASE-8410) - Restrict the amount of resources (i.e. regions, tables) a namespace can consume.
 * Namespace Security Administration (HBASE-9206) - Provide another level of security administration for tenants.
 * Region server groups (HBASE-6721) - A namespace/table can be pinned onto a subset of RegionServers thus guaranteeing a course level of isolation.

.Table

    An HBase table consists of multiple rows.

.Row

    A row in HBase consists of a row key and one or more columns with values associated with them. Rows are sorted alphabetically by the row key as they are stored. For this reason, the design of the row key is very important. The goal is to store data in such a way that related rows are near each other. A common row key pattern is a website domain. If your row keys are domains, you should probably store them in reverse (org.apache.www, org.apache.mail, org.apache.jira). This way, all of the Apache domains are near each other in the table, rather than being spread out based on the first letter of the subdomain.
Column

.Rowkey 

	Row keys are uninterpreted bytes. Rows are lexicographically sorted with the lowest order appearing first in a table. The empty byte array is used to denote both the start and end of a tables' namespace.

.Column

    A column in HBase consists of a column family and a column qualifier, which are delimited by a : (colon) character.
Column Family

.ColumnFamily

    Columns in Apache HBase are grouped into column families. All column members of a column family have the same prefix. For example, the columns courses:history and courses:math are both members of the courses column family. The colon character (:) delimits the column family from the column family qualifier. The column family prefix must be composed of printable characters. The qualifying tail, the column family qualifier, can be made of any arbitrary bytes. Column families must be declared up front at schema definition time whereas columns do not need to be defined at schema time but can be conjured on the fly while the table is up and running.
Physically, all column family members are stored together on the filesystem. Because tunings and storage specifications are done at the column family level, it is advised that all column family members have the same general access pattern and size characteristics.
Column families physically colocate a set of columns and their values, often for performance reasons. Each column family has a set of storage properties, such as whether its values should be cached in memory, how its data is compressed or its row keys are encoded, and others. Each row in a table has the same column families, though a given row might not store anything in a given column family.
Column Qualifier

.ColumnQualifier

    A column qualifier is added to a column family to provide the index for a given piece of data. Given a column family content, a column qualifier might be content:html, and another might be content:pdf. Though column families are fixed at table creation, column qualifiers are mutable and may differ greatly between rows.
Cell

.Cell

    A cell is a combination of row, column family, and column qualifier, and contains a value and a timestamp, which represents the value’s version.
Timestamp. The HBase version dimension is stored in decreasing order, so that when reading from a store file, the most recent values are found first.


.Timestamp

    A timestamp is written alongside each value, and is the identifier for a given version of a value. By default, the timestamp represents the time on the RegionServer when the data was written, but you can specify a different timestamp value when you put data into the cell.

	
[TIP]
====
The maximum number of versions to store for a given column is part of the column schema and is specified at table creation, or via an alter command, via HColumnDescriptor.DEFAULT_VERSIONS. 
==== 

[TIP]
.Modify the Maximum Number of Versions for a Column Family
====
alter ‘t1′, NAME => ‘f1′, VERSIONS => 5
====
 
[TIP]
.Modify the Maximum Number of Versions for a Column Family
====
alter ‘t1′, NAME => ‘f1′, MIN_VERSIONS => 2
====	
	
==  key-hashing strategy

Readings : 
 * https://sematext.com/blog/2012/04/09/hbasewd-avoid-regionserver-hotspotting-despite-writing-records-with-sequential-keys/
 * https://www.slideshare.net/amansk/hbase-schema-design-big-data-techcon-boston
	
==== Problems

The critical issue of distributing your row keys well to avoid “hot” regions is well known.
As an example of not really well distributed tables, let’s assume you need to store per-user data. 
In this case, the row key would simply be the user’s ID, which will probably be a monotonically increasing integer (i.e. generated using a sequence, using MySQL or other tools). It’s easy to see that having the user ID as the key would make all writes for new users go into the last region of your table, which handles the highest values. Additionally, if new users tend to be significantly more active than older ones, or vice-versa, then any updates to existing rows won’t be well distributed across regions as well. Substitute the term ‘User ID’ with ‘Ticket ID’, ‘Product ID’ or any other entity type where there’s a much higher than average write-rate for a small portion of IDs, and you might discover this issue in your own use-cases. 
	
==== Simple solution
However, if we examine that monotonic ID’s structure more closely, we’ll see that it does contain an element that cycles nicely and evenly with each new ID allocated: its least-significant byte. However, once you have any significant number of users, the most significant bytes of generated IDs remain pretty constant for long periods of time while only the least significant bytes rotate (I’m assuming big-endian order, which is pretty much the standard for binary serialization; see for example Hadoop’s Bytes class and Java’s DataOutputStream). This is unfortunate, because good distribution of keys relies on their most significant byte(s). To better illustrate this, think of the odometer in your car, whether digital or old school: how often would you see any of the left-most digits rotate?

speedometer
A fix for making these row keys distribute nicely is fairly easy to implement. You simply need to prefix the key with a leading byte based on the user ID, whose value is well distributed. In other words, you need a consistent hash. For any given ID, you should always get back the same value.

One way of achieving this is to define a fixed number of buckets, with the leading byte in the key being the bucket number. That byte is usually calculated as userId % BUCKETS_NUMBER. This in effect relies on the well distributed nature of the lowest byte, so alternatively you could just grab the whole least significant byte of the ID as the prefix.

If you have a table for collecting a user’s raw events for later per-user aggregation, then having a consistent hash also has another advantage. It guarantees that a user’s data will always reside under the same prefix, so you can write concurrent code that processes each prefix (i.e. each block of users) without the need for a later reduce phase between tasks. Of course, a MapReduce job could easily merge a user’s rows using the user ID as key, however at Dynamic Yield we’ve tried to steer clear from M/R when dealing with jobs that need to run frequently and quickly, given the high overhead of launching MapReduce. (We’re currently switching to Apache Spark for this kind of job). Whether you use M/R or custom parallel code, you probably want to ensure each task gets an equal share of work, which is another advantage of well distributed tables.

However, sometimes there’s no need for a consistent hash. Assume you have a table whose native key is simply the timestamp (for later scanning by time range). To avoid one hot region that handles all new writes, you could simply generate a random byte (with a value smaller than BUCKETS_NUMBER)as the prefix when writing a new row. To then perform a partial scan for any given time range, you would need a separate scan for each prefix. Note that this multi-scan approach significantly differs from using the built-in scan.setTimeRange() method to find all data with a given HBase-timestamp range (regardless of the row key). The latter requires the Region Server to perform intense analysis over much of the table’s data in order to filter out any data not in range. For large tables, this might mean a very slow scan.
	
	
=== Secondary indexes

TODO: HBase FuzzyRowFilter: Alternative to Secondary Indexes	
ref : https://sematext.com/blog/2012/08/09/consider-using-fuzzyrowfilter-when-in-need-for-secondary-indexes-in-hbase/


=== DBA Tips

.Activate compression :

  ALTER TABLE 'test', {NAME=>'mycolumnfamily', COMPRESSION=>'SNAPPY'} 

.Data block encoding of keys/values

 ALTER TABLE 'test', {NAME=>'mycolumnfamily', DATA_BLOCK_ENCODING => 'FAST_DIFF'}

.Change Split policy for a table (for Hbase 0.94+ the default Split policy changed from ConstantSizeRegionSplitPolicy (based on hbase.hregion.max.filesize) to IncreasingToUpperBoundRegionSplitPolicy)

 alter 'access_demo', {METHOD => 'table_att', CONFIGURATION => {'SPLIT_POLICY' => 'org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy'}}

Remember split will occur if the data size of a ColumnFamily gets bigger than the number defined by the policy.
  
== Some rules


There are many different data sets, with different access patterns and service-level expectations. Therefore, these rules of thumb are only an overview. Read the rest of this chapter to get more details after you have gone through this list.

 * Aim to have regions sized between 10 and 50 GB.
 * Aim to have cells no larger than 10 MB, or 50 MB if you use mob. Otherwise, consider storing your cell data in HDFS and store a pointer to the data in HBase.
 * A typical schema has between 1 and 3 column families per table. HBase tables should not be designed to mimic RDBMS tables.
 * Around 50-100 regions is a good number for a table with 1 or 2 column families. Remember that a region is a contiguous segment of a column family.
 * Keep your column family names as short as possible. The column family names are stored for every value (ignoring prefix encoding). They should not be self-documenting and descriptive like in a typical RDBMS.
 * If you are storing time-based machine data or logging information, and the row key is based on device ID or service ID plus time, you can end up with a pattern where older data regions never have additional writes beyond a certain age. In this type of situation, you end up with a small number of active regions and a large number of older regions which have no new writes. For these situations, you can tolerate a larger number of regions because your resource consumption is driven by the active regions only.
 * If only one column family is busy with writes, only that column family accomulates memory. Be aware of write patterns when allocating resources.

  
  
  
== Installation TIPS

[TIP]
====
 . ssh
 . dns
 . loopback entry
 . ntp
 . ulimit
====

=== _ssh_

HBase uses the Secure Shell (ssh) command and utilities extensively to communicate between cluster nodes. Each server in the cluster must be running ssh so that the Hadoop and HBase daemons can be managed. You must be able to connect to all nodes via SSH, including the local node, from the Master as well as any backup Master, using a shared key rather than a password. You can see the basic methodology for such a set-up in Linux or Unix systems at "Procedure: Configure Passwordless SSH Access". If your cluster nodes use OS X, see the section, SSH: Setting up Remote Desktop and Enabling Self-Login on the Hadoop wiki.

=== _DNS_

HBase uses the local hostname to self-report its IP address. Both forward and reverse DNS resolving must work in versions of HBase previous to 0.92.0. The hadoop-dns-checker tool can be used to verify DNS is working correctly on the cluster. The project README file provides detailed instructions on usage.

=== _Loopback IP_

Prior to hbase-0.96.0, HBase only used the IP address 127.0.0.1 to refer to localhost, and this could not be configured. See Loopback IP for more details.

=== _ NTP_

The clocks on cluster nodes should be synchronized. A small amount of variation is acceptable, but larger amounts of skew can cause erratic and unexpected behavior. Time synchronization is one of the first things to check if you see unexplained problems in your cluster. It is recommended that you run a Network Time Protocol (NTP) service, or another time-synchronization mechanism, on your cluster, and that all nodes look to the same service for time synchronization. See the Basic NTP Configuration at The Linux Documentation Project (TLDP) to set up NTP.===_Limits on Number of Files and Processes (ulimit)_

=== _ulimit_
Apache HBase is a database. It requires the ability to open a large number of files at once. Many Linux distributions limit the number of files a single user is allowed to open to 1024 (or 256 on older versions of OS X). You can check this limit on your servers by running the command ulimit -n when logged in as the user which runs HBase. See the Troubleshooting section for some of the problems you may experience if the limit is too low. You may also notice errors such as the following:
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901

It is recommended to raise the ulimit to at least 10,000, but more likely 10,240, because the value is usually expressed in multiples of 1024. Each ColumnFamily has at least one StoreFile, and possibly more than six StoreFiles if the region is under load. The number of open files required depends upon the number of ColumnFamilies and the number of regions. The following is a rough formula for calculating the potential number of open files on a RegionServer.
Calculate the Potential Number of Open Files

   (StoreFiles per ColumnFamily) x (regions per RegionServer)

For example, assuming that a schema had 3 ColumnFamilies per region with an average of 3 StoreFiles per ColumnFamily, and there are 100 regions per RegionServer, the JVM will open 3 * 3 * 100 = 900 file descriptors, not counting open JAR files, configuration files, and others. Opening a file does not take many resources, and the risk of allowing a user to open too many files is minimal.

Another related setting is the number of processes a user is allowed to run at once. In Linux and Unix, the number of processes is set using the ulimit -u command. This should not be confused with the nproc command, which controls the number of CPUs available to a given user. Under load, a ulimit -u that is too low can cause OutOfMemoryError exceptions. See Jack Levin's major HDFS issues thread on the hbase-users mailing list, from 2011.





= Programming with hbase



== Shell

=== shell from text file

You can enter HBase Shell commands into a text file, one command per line, and pass that file to the HBase Shell.

.Example command file

====
 create 'test', 'cf'
 list 'test'
 put 'test', 'row1', 'cf:a', 'value1'
 put 'test', 'row2', 'cf:b', 'value2'
 put 'test', 'row3', 'cf:c', 'value3'
 put 'test', 'row4', 'cf:d', 'value4'
 scan 'test'
 get 'test', 'row1'
 disable 'test'
 enable 'test'
====

.run command from file
====
 ./hbase shell ./sample_commands.txt
====


===  A detailed documentation of the commands is available here

https://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/


=== Beining efficient with the shell 

==== configuration file 

irbrc file-irbrc configuration to save all command history of all hbase shell invocations.


.minimal configuration of irbrc-

[source]
----
more ~/.irbrc
require 'irb/ext/save-history'
IRB.conf[:SAVE_HISTORY] = 100
IRB.conf[:HISTORY_FILE] = "#{ENV['HOME']}/.irb_history"
Kernel.at_exit do
    IRB.conf[:AT_EXIT].each do |i|
        i.call
    end
end
----

==== enabling debug model

[source]
-----
hbase>debug
or
./bin/hbase shell -d
-----

==== counters

counters with hbase- hbase offers counter feature, counters are very useful in statistics


[source]
-----
hbase(main):001:0> create 'account', 'id'
0 row(s) in 1.1930 seconds
hbase(main):002:0> incr 'account', '2014', 'id:n', 1
COUNTER VALUE = 1
hbase(main):04:0> get_counter 'account', '2014', 'id:n'
COUNTER VALUE = 2
-----

==== avoid full scan row  : scan query optimization


Scan is used to get the data from hbase and the costliest operation.
An optional startRow and stopRow is useful to improve the query performance.If rows are not defined(start and stop), the Scanner will iterate over all rows.
Hbase scan queries with start and end key are much faster because, it doesn’t have to scan everything to get the specified query/filter data.
Here is tricks-

[source]
-----
    create hbase table and populate data-

    create 'TS','cf'
-----

the result will be 
.Table populated
|===
|card_number_year_month_day_time_o |transaction_amt|location|type|year|month

|100_2014_06_10_10_932845_ta
|100
|bangalore
|credit
|2014
|6

|23989_2000_01_11_10_5468756_ta
|45843745
|bangalore india
|debit
|2000
|5

|487545_2000_01_11_10_5468756_ta
|
|
|
|2000
|1
|===


Avoid Full Table Scan-

find out all transaction done by card number x at place bangalore.
use prefix/rowkey filter with regex/substring comparator to set the search condition and set the start row as ‘X’ and stop row ‘X~’.
Row keys are sorted(lexical) and data is stored in byte in hbase. The start/stop key helps to avoid the complete table scan and fetch the data from region contains the range value, as(~) is last in ascii table so hbase scan lookup the rows having prefix X~.
Retrieving data from HBase scan with filter-

[source]
-----
    Scan scan = new Scan(Bytes.ToBytes("23989"),Bytes.toBytes("23989~");
    scan.setFilter(...);
-----

Disable cache at client-

	
[source]
-----
    setCacheBlocks(false)
    and setCaching(0) 
-----

Get all the row having account number 23989


[source]
-----
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.RowFilter
import org.apache.hadoop.hbase.filter.SubstringComparator
scan 'TS', {STARTROW=>'23989', STOPROW=>'23989~',FILTER=>RowFilter.new(CompareFilter::CompareOp.valueOf('EQUAL'),SubstringComparator.new('23989'))}
-----

Use start and stop row to optimize scan query.


== Java


There's several good ressources to start with available 

 * http://www.informit.com/articles/article.aspx?p=2255108&seqNum=2
 * https://autofei.wordpress.com/2012/04/02/java-example-code-using-hbase-data-model-operations/
 * https://www.tutorialspoint.com/hbase/hbase_read_data.htm

=== Let's start with maven

.pom file

[source,xml]
-----
<project xmlns="http://maven.apache.org/POM/4.0.0" 
	     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.zenika</groupId>
    <artifactId>hbase-example</artifactId>
    <version>1.0-SNAPSHOT</version>
    <packaging>jar</packaging>

    <name>hbase-example</name>
    <url>http://maven.apache.org</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
             <groupId>org.apache.hbase</groupId>
             <artifactId>hbase-client</artifactId>
             <version>0.98.5-hadoop2</version>
        </dependency>

        <dependency>
             <groupId>junit</groupId>
             <artifactId>junit</artifactId>
             <version>4.11</version>
             <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.0.2</version>
                <configuration>
                    <source>1.6</source>
                    <target>1.6</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <configuration>
                   <archive>
                       <manifest>
                           <addClasspath>true</addClasspath>
                           <classpathPrefix>lib/</classpathPrefix>
                           <mainClass>com.zenika.hbaseexample.HBaseExample</mainClass>
                       </manifest>
                    </archive>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-dependency-plugin</artifactId>
                <executions>
                    <execution>
                        <id>copy</id>
                        <phase>install</phase>
                        <goals>
                            <goal>copy-dependencies</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}/lib</outputDirectory>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
-----

.compile install and run 

[source,shell]
-----
   mvn clean install
-----

This creates a file in the target directory named hbase-example-1.0-SNAPSHOT.jar. You can execute it with the following command:

[source,shell]
-----
  java -jar hbase-example-1.0-SNAPSHOT.jar
-----

=== The POJO page view

[source,java]
-----
package com.zenika.hbaseexample;

public class PageView
{

    private String userId;
    private String page;

    public PageView() {
    }

    public PageView(String userId, String page) {
        this.userId = userId;
        this.page = page;

    }
    public String getUserId() {
        return userId;
    }

    public void setUserId(String userId) {
        this.userId = userId;
    }
    public String getPage() {
        return page;
    }

    public void setPage(String page) {
        this.page = page;
    }

}
-----

=== The main class

[source,java]
-----
package com.zenika.hbaseexample;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class HBaseExample
{

    private HTableInterface pageViewTable;

    public HBaseExample()
    {
        try
        {
            Configuration conf = HBaseConfiguration.create();
			//you may need this specific configuration
			//conf.set("hbase.zookeeper.quorum", "server’s IP address");
            pageViewTable = new HTable( conf, "PageViews");
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
    }

    public void close()
    {
        try

        {
            pageViewTable.close();
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
    }

    public void put( PageView pageView )
    {
        // Create a new Put object with the Row Key as the bytes of the user id
        Put put = new Put( Bytes.toBytes( pageView.getUserId() ) );

        // Add the user id to the info column family
        put.add( Bytes.toBytes( "info" ),
                 Bytes.toBytes( "userId" ),
                 Bytes.toBytes( pageView.getUserId() ) );

        // Add the page to the info column family
        put.add( Bytes.toBytes( "info" ),
                 Bytes.toBytes( "page" ),
                 Bytes.toBytes( pageView.getPage() ) );
        try

        {

            // Add the PageView to the page view table
            pageViewTable.put( put );
        }
        catch( IOException e )
        {
            e.printStackTrace();
        }
    }

    public PageView get( String rowkey )

    {
        try
        {

            // Create a Get object with the rowkey (as a byte[])
            Get get = new Get( Bytes.toBytes( rowkey ) );

            // Execute the Get
            Result result = pageViewTable.get( get );

            // Retrieve the results
            PageView pageView = new PageView();
            byte[] bytes = result.getValue( Bytes.toBytes( "info" ),
                                            Bytes.toBytes( "userId" ) );
            pageView.setUserId( Bytes.toString( bytes ) );
            bytes = result.getValue( Bytes.toBytes( "info" ),
                                     Bytes.toBytes( "page" ) );
            pageView.setPage(Bytes.toString(bytes));


            // Return the newly constructed PageView
            return pageView;
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
        return null;
    }
    public void delete( String rowkey )
    {
        try
        {
            Delete delete = new Delete( Bytes.toBytes( rowkey ) );
            pageViewTable.delete( delete );
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
    }

    public List<PageView> scan( String startRowKey, String endRowKey )
    {
        try
        {
            // Build a list to hold our results
            List<PageView> pageViewResults = new ArrayList<PageView>();


            // Create and execute a scan
            Scan scan = new Scan( Bytes.toBytes( startRowKey ), Bytes.toBytes( endRowKey ) );
            ResultScanner results = pageViewTable.getScanner(scan);

            for( Result result : results )

            {
                // Build a new PageView
                PageView pageView = new PageView();
                byte[] bytes = result.getValue( Bytes.toBytes( "info" ),
                        Bytes.toBytes( "userId" ) );
                pageView.setUserId( Bytes.toString( bytes ) );
                bytes = result.getValue( Bytes.toBytes( "info" ),
                        Bytes.toBytes( "page" ) );
                pageView.setPage(Bytes.toString(bytes));

                // Add the PageView to our results
                pageViewResults.add( pageView );
            }

            // Return our results
            return pageViewResults;
        }
        catch (IOException e)
        {
            e.printStackTrace();
        }
        return null;
    }

    public static void main( String[] args )

    {
        HBaseExample example = new HBaseExample();

        // Create two records
        example.put( new PageView( "User1", "/mypage" ) );
        example.put( new PageView( "User2","/mypage" ) );

        // Execute a Scan from "U" to "V"
        List<PageView> pageViews = example.scan( "U", "V" );
        if( pageViews != null ) {
            System.out.println("Page Views:");
            for (PageView pageView : pageViews) {
                System.out.println("\tUser ID: " + pageView.getUserId() + ", Page: " + pageView.getPage());
            }
        }

        // Get a specific row
        PageView pv = example.get( "User1" );
        System.out.println( "User ID: " + pv.getUserId() + ", Page: " + pv.getPage() );

        // Delete a row
        example.delete( "User1" );

        // Execute another scan, which should just have User2 in it
        pageViews = example.scan( "U", "V" );
        if( pageViews != null ) {
            System.out.println("Page Views:");
            for (PageView pageView : pageViews) {
                System.out.println("\tUser ID: " + pageView.getUserId() + ", Page: " + pageView.getPage());
            }
        }

        // Close our table
        example.close();
    }
}
-----

== PHP

== Toolings

==== YCSB

https://github.com/brianfrankcooper/YCSB/wiki


