= Hadoop

:toc:


= Introduction

The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. 

 * _Hadoop Common_: The common utilities that support the other Hadoop modules. 
 * _Hadoop Distributed File System (HDFS™)_: A distributed file system that provides high-throughput access to application data.
 * _Hadoop YARN_: A framework for job scheduling and cluster resource management.
 * _Hadoop MapReduce_: A YARN-based system for parallel processing of large data sets.

== Version 
 
Current is 2.7.3  , 3.0 is comming.

.What will be new in 3.0 :
 * Minimum required Java version increased from Java 7 to Java 8
 * Support for erasure encoding in HDFS : Erasure coding is a method for durably storing data with significant space savings compared to replication. Standard encodings like Reed-Solomon (10,4) have a 1.4x space overhead, compared to the 3x overhead of standard HDFS replication.
 * YARN Timeline Service v.2: addresses two major challenges: improving scalability and reliability of Timeline Service, and enhancing usability by introducing flows and aggregation.
 * Shell script rewrite
 * Shaded client jars
 * Support for Opportunistic Containers and Distributed Scheduling.
 * MapReduce task-level native optimization
 * Support for more than 2 NameNodes.
 * Default ports of multiple services have been changed.
 * Hadoop now supports integration with Microsoft Azure Data Lake and Aliyun Object Storage System as alternative Hadoop-compatible filesystems.
 * Intra-datanode balancer

= Let's start 

== Installation

 * Download from apache location (2.7.3), http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz  or for 3.0 http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.0.0-alpha1/hadoop-3.0.0-alpha1.tar.gz
 * unpack, 
 * edit etc/hadoop/hadoop-env.sh to define the javahome that match your installation

== run it

 [root@localhost hadoop]# bin/hadoop
 Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar <jar>            run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

== your firsts "map reduce" operations

=== Count the occurence number of dfs .. in a jvm 

  $ mkdir input
  $ cp etc/hadoop/*.xml input
  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+'
  $ cat output/*

=== Do the same with Zookeeper

... what do you notice ... output ..... 

=== Pseudo-Distributed Operation

==== Configuration

in etc/hadoop/core-site.xml:

 <configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
 </configuration>

in etc/hadoop/hdfs-site.xml:

 <configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
 </configuration>
 
==== Setup ssh 

 check you can "ssh localhost" without passphrase.
 if not : 
 
  $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keys

  
==== Create DFS partition, create and start NameNode DataNode

* NameNode

* DataNode

* Yarn 

==== firewall setting for centos 7

 sudo firewall-cmd --get-active-zones
 sudo firewall-cmd --zone=public --add-port=9000/tcp --permanent
 sudo firewall-cmd --zone=public --add-port=50070/tcp --permanent
 sudo firewall-cmd --reload
 
==== LEt's start name node and data node

  bin/hdfs namenode -format
  sbin/start-dfs.sh

from here you can check the webui  http://localhost:50070/

image::dfshealth.png[dfs GUI]
.dfs status
   
We can create the bases repositories
   
 $ bin/hdfs dfs -mkdir /user
 $ bin/hdfs dfs -mkdir /user/<username>

Then Copy the input files into the distributed filesystem:

  $ bin/hdfs dfs -put etc/hadoop input

Run some of the examples provided:

  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+'

Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:

  $ bin/hdfs dfs -get output output
  $ cat output/*   

We can also browse the ui

image::dfs_browseFileSystem.png[Browse the file system]   

Or get the information directly from hdfs

 bin/hdfs dfs -cat output/*
 
that gave 

 [admin@localhost hadoop]$  bin/hdfs dfs -cat output/*6	dfs.audit.logger
 4	dfs.class
 3	dfs.server.namenode.
 2	dfs.period
 2	dfs.audit.log.maxfilesize
 2	dfs.audit.log.maxbackupindex
 1	dfsmetrics.log
 1	dfsadmin
 1	dfs.servers
 1	dfs.replication
 1	dfs.file
 
 

When you’re done, stop the daemons with:
sbin/stop-dfs.sh   

=== let's do it with yarn

Configure parameters as follows:etc/hadoop/mapred-site.xml:

 <configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
 </configuration>

etc/hadoop/yarn-site.xml:

 <configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
 </configuration>

Start ResourceManager daemon and NodeManager daemon:

 $ sbin/start-dfs.sh
 $ sbin/start-yarn.sh

Browse the web interface for the ResourceManager; by default it is available at:
ResourceManager - http://localhost:8088/

image::yarn_startup.png[Yarn GUI]

Run a MapReduce job.

 bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+'

image::yarn_running.png[Yarn running]

When you’re done, stop the daemons with:

  $ sbin/stop-yarn.sh and sbin/stop-dfs.sh

  
== Formalisation and a step further : Cluster

=== Configuration files


.Hadoop’s Java configuration is driven by two types of important configuration files:

 Read-only default configuration - core-default.xml, hdfs-default.xml, yarn-default.xml and mapred-default.xml.

 Site-specific configuration - etc/hadoop/core-site.xml, etc/hadoop/hdfs-site.xml, etc/hadoop/yarn-site.xml and etc/hadoop/mapred-site.xml.

 Additionally, you can control the Hadoop scripts found in the bin/ directory of the distribution, by setting site-specific values via the etc/hadoop/hadoop-env.sh and etc/hadoop/yarn-env.sh.

To configure the Hadoop cluster you will need to configure the environment in which the Hadoop daemons execute as well as the configuration parameters for the Hadoop daemons.
HDFS daemons are NameNode, SecondaryNameNode, and DataNode. YARN damones are ResourceManager, NodeManager, and WebAppProxy. If MapReduce is to be used, then the MapReduce Job History Server will also be running. For large installations, these are generally running on separate hosts.


Full example is here : 
http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/ClusterSetup.html


Web Interfaces
Once the Hadoop cluster is up and running check the web-ui of the components as described below:



[format="csv", options="header"]
|===
Daemon,Web Interface,Notes
NameNode,http://nn_host:port/ ,Default HTTP port is 50070 
ResourceManager ,http://rm_host:port/ ,Default HTTP port is 8088
MapReduce JobHistory Server ,http://jhs_host:port/ ,Default HTTP port is 19888
|===
